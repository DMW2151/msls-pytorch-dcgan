{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c8fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived from the DCGAN paper's Parzen Estimation LL calculations:\n",
    "#\n",
    "# Fundamentally, no changes to the method; some updates for Python2 -> Python3.7+, adding\n",
    "# verbose comments on LL methods, cutting CLI wrapper, and handling for sending Pytorch.datasets\n",
    "# data to Theano\n",
    "#\n",
    "# See: https://github.com/goodfeli/adversarial/blob/master/parzen_ll.py\n",
    "\n",
    "# NLL Functions from DCGAN Paper: Credit: Yann N. Dauphin\n",
    "\n",
    "# NOTE: On Sagemaker, use `conda_amazonei_pytorch_latest_p37` (OR `conda_pytorch_latest_p36`)\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import gc\n",
    "import datetime\n",
    "\n",
    "# Torch Deps\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# DCGAN\n",
    "import gaudi_dcgan as dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Simple wrapper around torch.utils.data.Dataset to limit # of data-points passed\n",
    "    to a DataLoader\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, n):\n",
    "        self.dataset = dataset\n",
    "        self.n = n\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Clobber the old Length\"\"\"\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd198a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nll(x, parzen, batch_size=10):\n",
    "    \"\"\"\n",
    "    Calculate the Negative Log-Liklihood over X using parzen function\n",
    "    -------\n",
    "    Args:\n",
    "        X -\n",
    "        parzen - theano.function, see `theano_parzen`\n",
    "        batch_size - int - # of images to use for each NLL sample\n",
    "    \"\"\"\n",
    "\n",
    "    inds = range(x.shape[0])\n",
    "    n_batches = int(np.ceil(float(len(inds)) / batch_size))\n",
    "    nlls = []\n",
    "    for i in range(n_batches):\n",
    "        nll = parzen(x[inds[i::n_batches]])\n",
    "        nlls.extend(nll)\n",
    "        if i % 10 == 0:\n",
    "            print(\n",
    "                f\"[{datetime.datetime.utcnow().__str__()}]\\t[{i}/{n_batches}]\\tMean NLL: {np.mean(nlls)}\"\n",
    "            )\n",
    "\n",
    "    return np.array(nlls)\n",
    "\n",
    "\n",
    "def log_mean_exp(a):\n",
    "    max_ = a.max(1)\n",
    "    return max_ + T.log(T.exp(a - max_.dimshuffle(0, \"x\")).mean(1))\n",
    "\n",
    "\n",
    "def theano_parzen(mu, sigma):\n",
    "    \"\"\"\n",
    "    Create Parzen function from sample of Mu (i.e. Samples from G)\n",
    "    -------\n",
    "    Args:\n",
    "        - mu - np.Array - Samples from G cast to NDArray and reshaped\n",
    "        - sigma - float32 - proposed sigma value for Parzen Kernel\n",
    "    \"\"\"\n",
    "\n",
    "    x = T.matrix()\n",
    "    mu = theano.shared(mu)\n",
    "\n",
    "    a = (x.dimshuffle(0, \"x\", 1) - mu.dimshuffle(\"x\", 0, 1)) / sigma\n",
    "    E = log_mean_exp(-0.5 * (a ** 2).sum(2))\n",
    "    Z = mu.shape[1] * T.log(sigma * np.sqrt(np.pi * 2))\n",
    "\n",
    "    return theano.function([x], E - Z)\n",
    "\n",
    "\n",
    "def cross_validate_sigma(g_samples, data, sigmas, batch_size):\n",
    "    \"\"\"\n",
    "    Select optimal kernel size for Parzen\n",
    "    -------\n",
    "    Args:\n",
    "        g_samples - numpy.ndarray - Sample images from G\n",
    "        data - numpy.ndarray - Sample images from MSLS\n",
    "        sigmas - numpy.ndarray - array of sigmas to test\n",
    "    \"\"\"\n",
    "\n",
    "    lls = []\n",
    "    for sigma in sigmas:\n",
    "        print(f\"[{datetime.datetime.utcnow().__str__()}]\\t[σ = {sigma}]\")\n",
    "\n",
    "        parzen = theano_parzen(g_samples, sigma)\n",
    "        tmp = get_nll(data, parzen, batch_size=batch_size)\n",
    "\n",
    "        lls.append(np.asarray(tmp).mean())\n",
    "        del parzen\n",
    "        gc.collect()\n",
    "\n",
    "    ind = np.argmax(lls)\n",
    "    print(f\"[{datetime.datetime.utcnow().__str__()}]\\t[Using: σ = {sigma}]\")\n",
    "    return sigmas[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54674f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset...\n",
    "\n",
    "# Inputs\n",
    "DATAROOT = \"/efs/samples\"\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "DATASET_SIGMA = None\n",
    "\n",
    "# See Section `Data and Translations` for discussion on what this dataloader\n",
    "# sequence does\n",
    "\n",
    "dataset = dset.ImageFolder(\n",
    "    root=DATAROOT,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.2, 0.0)),\n",
    "            transforms.CenterCrop(IMG_SIZE * 4),\n",
    "            transforms.Resize(IMG_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                ),\n",
    "                (\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Use LimitDataset wrapper to ensure this doesn't blow up memory...\n",
    "limited_msls_data = LimitDataset(dataset, 2000)\n",
    "\n",
    "# WARNING: Create the dataloader - We're just going to have it load in a single iteration\n",
    "# I *KNOW* we're working on a small dataset (<2,000 imgs), but this is generally a bad idea!!\n",
    "msls_real_data = torch.utils.data.DataLoader(\n",
    "    limited_msls_data,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    batch_size=len(dataset),\n",
    ")\n",
    "\n",
    "msls_real_data = next(iter(msls_real_data))[0].numpy()\n",
    "print(f\"Shape after Fetch From Loader: {msls_real_data.shape}\")\n",
    "\n",
    "msls_real_data = msls_real_data.reshape(\n",
    "    (msls_real_data.shape[0], np.prod(msls_real_data.shape[1:]))\n",
    ")\n",
    "\n",
    "print(f\"Shape after Reshape: {msls_real_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ac462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data From G(Z)\n",
    "model_cfg = dcgan.ModelCheckpointConfig()\n",
    "train_cfg = dcgan.TrainingConfig()\n",
    "\n",
    "# generated_data.shape == torch.Size([16, 3, 64, 64]);\n",
    "# but then convert to numpy for Theano! -> (16, 3, 64, 64)\n",
    "generated_data = dcgan.generate_fake_samples(\n",
    "    n_samples=16, train_cfg=train_cfg, model_cfg=model_cfg, as_of_epoch=4\n",
    ").numpy()\n",
    "\n",
    "print(f\"Shape after Generation: {generated_data.shape}\")\n",
    "\n",
    "# Reshape for Theano!\n",
    "generated_data = generated_data.reshape(\n",
    "    (generated_data.shape[0], np.prod(generated_data.shape[1:]))\n",
    ")\n",
    "\n",
    "print(f\"Shape after Reshape: {generated_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40003ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are comfortable estimating sigma for the Gaussian (or have estimated it before), then\n",
    "# skip sigma estimation\n",
    "\n",
    "if DATASET_SIGMA:\n",
    "    sigma = DATASET_SIGMA\n",
    "else:\n",
    "    sigma = cross_validate_sigma(\n",
    "        generated_data,\n",
    "        msls_real_data,\n",
    "        np.logspace(-1.0, 0, num=10),  # Default Sigma Space...\n",
    "        BATCH_SIZE,  # Default Batch Size\n",
    "    )\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# fit and evaulate\n",
    "parzen = theano_parzen(generated_data, sigma)\n",
    "\n",
    "ll = get_nll(msls_real_data, parzen, batch_size=BATCH_SIZE)\n",
    "\n",
    "se = ll.std() / np.sqrt(msls_real_data.shape[0])\n",
    "\n",
    "print(f\"Log-Likelihood of Test Set = {ll.mean()}, se: {se}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
