{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is a wrapper around gaudi_dcgan.py and is used to render outputs from\n",
    "# the model (e.g. generated images, loss plots, etc.)\n",
    "\n",
    "# NOTE: On Sagemaker, use `conda_amazonei_pytorch_latest_p37` (OR `conda_pytorch_p36`)\n",
    "\n",
    "# General Deps\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "import ffmpeg\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Torch Deps\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# DCGAN\n",
    "import gaudi_dcgan as dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample Usage on Command Line - See Notes on Running on DL1\n",
    "#\n",
    "# ! python3 run_gaudi_dcgan.py \\\n",
    "#    --dataroot \"/efs/images/\" \\\n",
    "#    --seed 215 \\\n",
    "#    --name msls_2022_01_24_001 \\\n",
    "#    --s_epoch 0 \\\n",
    "#    --n_epoch 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec446409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed Model\n",
    "random.seed(215)\n",
    "torch.manual_seed(215)\n",
    "\n",
    "# Init Model Config w. Default DCGAN Values\n",
    "model_cfg = dcgan.ModelCheckpointConfig()\n",
    "train_cfg = dcgan.TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# We can use an image folder dataset...\n",
    "dataroot = \"/efs/images/\"\n",
    "\n",
    "# See Section `Data and Translations` for discussion on what this dataloader\n",
    "# sequence does\n",
    "dataset = dset.ImageFolder(\n",
    "    root=dataroot,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.2, 0.0)),\n",
    "            transforms.CenterCrop(train_cfg.img_size * 4),\n",
    "            transforms.Resize(train_cfg.img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                ),\n",
    "                (\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create the dataloader with Similar Params to Habana\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    timeout=0,\n",
    "    batch_size=train_cfg.batch_size,\n",
    ")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        vutils.make_grid(\n",
    "            real_batch[0].to(train_cfg.dev)[:16], padding=2, normalize=True\n",
    "        ).cpu(),\n",
    "        (1, 2, 0),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create Figures Directory if Not Yet Exists\n",
    "if not os.path.exists(f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures\"):\n",
    "    os.makedirs(f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures\")\n",
    "\n",
    "plt.savefig(f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures/train_samples.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ec787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training\n",
    "result = dcgan.start_or_resume_training_run(\n",
    "    dataloader, train_cfg, model_cfg, n_epochs=16, st_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Start Datetime for plots; apply all the way through...\n",
    "plots_execution_dttm = re.sub(\":|-| |\\.\", \"_\", datetime.datetime.utcnow().__str__())\n",
    "\n",
    "# Plot the Losses Over Time\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(f\"Generator and Discriminator Loss During Training - {model_cfg.model_name}\")\n",
    "plt.plot(result[\"losses\"][\"_G\"], label=\"G\")\n",
    "plt.plot(result[\"losses\"][\"_D\"], label=\"D\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create Figures Directory if Not Yet Exists\n",
    "if not os.path.exists(f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures\"):\n",
    "    os.makedirs(f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures\")\n",
    "\n",
    "plt.savefig(\n",
    "    f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures/train_loss_{plots_execution_dttm}.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Graphic of the final images...\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Create a Frame for each epoch from results.img_list\n",
    "ims = [\n",
    "    [plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in result[\"img_list\"]\n",
    "]\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "# Plot a Video of the Final Training Progress Sequence\n",
    "content = HTML(ani.to_jshtml())\n",
    "\n",
    "writergif = animation.PillowWriter(\n",
    "    fps=10, metadata=dict({\"title\": f\"{model_cfg.model_name}\"}, **model_cfg.__dict__)\n",
    ")\n",
    "\n",
    "# Create Videos Directory if Not Yet Exists...\n",
    "if not os.path.exists(f\"{model_cfg.model_dir}/{model_cfg.model_name}/videos\"):\n",
    "    os.makedirs(f\"{model_cfg.model_dir}/{model_cfg.model_name}/videos\")\n",
    "\n",
    "# Save Animation as Gif and as HTML w. Video...\n",
    "ani.save(\n",
    "    f\"{model_cfg.model_dir}/{model_cfg.model_name}/videos/progress_{plots_execution_dttm}.gif\",\n",
    "    writer=writergif,\n",
    ")\n",
    "\n",
    "with open(\n",
    "    f\"{model_cfg.model_dir}/{model_cfg.model_name}/videos/progress_{plots_execution_dttm}.html\",\n",
    "    \"w\",\n",
    ") as fi:\n",
    "    print(ani.to_html5_video(), file=fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader and compare the final Generated images vs the Real\n",
    "# images. Do they hold up against human discretion?\n",
    "\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        vutils.make_grid(\n",
    "            real_batch[0].to(train_cfg.dev)[:64], padding=5, normalize=True\n",
    "        ).cpu(),\n",
    "        (1, 2, 0),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the fake images from the final epoch\n",
    "# NOTE: This uses the fixed noise from the trained model and is not\n",
    "# variable between executions\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images\")\n",
    "plt.imshow(np.transpose(result[\"img_list\"][-1], (1, 2, 0)))\n",
    "plt.show()\n",
    "\n",
    "# Create Figures Directory if Not Yet Exists...\n",
    "if not os.path.exists(f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures\"):\n",
    "    os.makedirs(f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures\")\n",
    "\n",
    "plt.savefig(\n",
    "    f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures/compare_{plots_execution_dttm}.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631fb0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a few sample images; this is using randomly generated noise\n",
    "# and results should be variable across multiple runs...\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# generated_data[0].shape == torch.Size([3, 64, 64])\n",
    "imgs = dcgan.generate_fake_samples(\n",
    "    n_samples=16, train_cfg=train_cfg, model_cfg=model_cfg, as_of_epoch=4\n",
    ")\n",
    "\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        vutils.make_grid(imgs.to(train_cfg.dev), padding=2, normalize=True).cpu(),\n",
    "        (1, 2, 0),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create Figures Directory if Not Yet Exists...\n",
    "if not os.path.exists(f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures\"):\n",
    "    os.makedirs(f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures\")\n",
    "\n",
    "plt.savefig(\n",
    "    f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures/generaed_{plots_execution_dttm}.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d96e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Ideally a Generator Net can use a CPU to (slowly) generate samples, being able to move\n",
    "# the G network off GPU/HPU allows us to serve imgs off an inexpensive webserver. In this case we generator\n",
    "# noise AND images on the CPU\n",
    "\n",
    "train_cfg_cpu_only = dcgan.TrainingConfig(dev=torch.device(\"cpu\"))\n",
    "\n",
    "imgs = dcgan.generate_fake_samples(\n",
    "    n_samples=4, train_cfg=train_cfg_cpu_only, model_cfg=model_cfg, as_of_epoch=4\n",
    ")\n",
    "\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        vutils.make_grid(imgs.to(train_cfg.dev), padding=2, normalize=True).cpu(),\n",
    "        (1, 2, 0),\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
