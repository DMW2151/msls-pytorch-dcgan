{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is a wrapper around gaudi_dcgan.py and is used to render outputs from \n",
    "# the model (e.g. generated images, loss plots, etc.)\n",
    "\n",
    "# NOTE: On Sagemaker, use `conda_amazonei_pytorch_latest_p37` (OR `conda_amazonei_pytorch_latest_p38`)\n",
    "    \n",
    "# General Deps\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Torch Deps\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# DCGAN\n",
    "import gaudi_dcgan as dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample Usage on Command Line - See Notes on Running on DL1\n",
    "#\n",
    "# ! python3 run_gaudi_dcgan.py \\\n",
    "#    --dataroot \"/efs/images/\" \\\n",
    "#    --seed 215 \\\n",
    "#    --name msls_2022_01_24_001 \\\n",
    "#    --s_epoch 0 \\\n",
    "#    --n_epoch 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec446409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed Model\n",
    "random.seed(215)\n",
    "torch.manual_seed(215)\n",
    "\n",
    "# Init Model Config w. Default DCGAN Values\n",
    "model_cfg = dcgan.ModelCheckpointConfig()\n",
    "train_cfg = dcgan.TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# We can use an image folder dataset; depending on the size of the training directory this can \n",
    "# take a little to instantiate; about 5-8 min for 25GB (also depends on EFS burst size)\n",
    "dataroot = \"/efs/images/\"\n",
    "\n",
    "# See Section `Data and Translations` for discussion on what this dataloader \n",
    "# sequence does\n",
    "dataset = dset.ImageFolder(\n",
    "    root=dataroot,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomAffine(degrees = 0, translate = (0.2, 0.0)),\n",
    "            transforms.CenterCrop(train_cfg.img_size * 4),\n",
    "            transforms.Resize(train_cfg.img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                ),\n",
    "                (\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create the dataloader with Similar Params to Habana\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    timeout=0,\n",
    "    batch_size=train_cfg.batch_size\n",
    ")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        vutils.make_grid(\n",
    "            real_batch[0].to(train_cfg.dev)[:16], padding=2, normalize=True\n",
    "        ).cpu(),\n",
    "        (1, 2, 0),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ec787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Training\n",
    "result = dcgan.start_or_resume_training_run(\n",
    "        dataloader, train_cfg, model_cfg, n_epochs=16, st_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Losses Over Time\n",
    "\n",
    "# X -> Training Step\n",
    "# Y -> Loss\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(result[\"losses\"][\"_G\"], label=\"G\")\n",
    "plt.plot(result[\"losses\"][\"_D\"], label=\"D\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Graphic of the final images...\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Create a Frame for each epoch from results.img_list\n",
    "ims = [\n",
    "    [\n",
    "        plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)\n",
    "    ] for i in result[\"img_list\"]\n",
    "]\n",
    "\n",
    "ani = animation.ArtistAnimation(\n",
    "    fig, ims, interval=1000, \n",
    "    repeat_delay=1000, blit=True\n",
    ")\n",
    "\n",
    "content = HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0610498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Video of the Final Training Progress Sequence\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader and compare the final Generated images vs the Real\n",
    "# images. Do they hold up against human discretion?\n",
    "\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        vutils.make_grid(\n",
    "            real_batch[0].to(train_cfg.dev)[:64], padding=5, normalize=True\n",
    "        ).cpu(),\n",
    "        (1, 2, 0),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the fake images from the final epoch\n",
    "# NOTE: This uses the fixed noise from the trained model and is not\n",
    "# variable between executions\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images\")\n",
    "plt.imshow(np.transpose(result['img_list'][-1], (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631fb0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a few sample images; this is using randomly generated noise\n",
    "# and results should be variable across multiple runs...\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "imgs = dcgan.generate_fake_samples(\n",
    "    n_samples=16,\n",
    "    train_cfg=train_cfg,\n",
    "    model_cfg=model_cfg,\n",
    "    as_of_epoch=16\n",
    ")\n",
    "\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        vutils.make_grid(imgs.to(train_cfg.dev), padding=2, normalize=True).cpu(),\n",
    "        (1, 2, 0),\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
