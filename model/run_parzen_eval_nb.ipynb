{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3221c15",
   "metadata": {},
   "source": [
    "# DCGAN - Parzen Window-based Log-Likelihood Estimates\n",
    "\n",
    "This notebook is a wrapper around the Parzen log-liklihood estimator described and implemented\n",
    "in the [original DCGAN paper](https://github.com/goodfeli/adversarial/blob/master/parzen_ll.py). \n",
    "\n",
    "> We estimate probability of the test set data under pg by fitting a Gaussian Parzen window to the\n",
    "samples generated with G and reporting the log-likelihood under this distribution. The Ïƒ parameter of the Gaussians was obtained by cross validation on the validation set. This procedure was intro-\n",
    "duced in Breuleux et al. [8] and used for various generative models for which the exact likelihood\n",
    "is not tractable\n",
    "\n",
    "Slight modifications are made in the local file (`parzen_ll.py`) for the following:\n",
    "\n",
    "- Migrate from Python2 -> Python3 syntax\n",
    "- Add comments and docstrings for clarity\n",
    "\n",
    "The goal of this project is not to develop a new  ramework for estimating generative models, consequently, the log-likelihoods calculated here are meant only for internal comparison between models. As you'll notice, I do not use `MNIST`, `TFD`, or `CIFAR-10` as a validation set, but rather a sample of MSLS images held-out from training.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are NOT on all `conda_amazonei_pytorch_latest_p3X` or `conda_pytorch_p3X` builds\n",
    "! pip3 install \\\n",
    "    tensorboard \\\n",
    "    theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import theano\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "# Torch Deps\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# DCGAN\n",
    "import gaudi_dcgan as dcgan\n",
    "import parzen_ll as plle\n",
    "import msls_dcgan_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55192da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Simple wrapper around torch.utils.data.Dataset to limit # of data-points\n",
    "    passed to a DataLoader; used to \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, n):\n",
    "        self.dataset = dataset\n",
    "        self.n = n\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Clobber the old Length\"\"\"\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ef253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs \n",
    "\n",
    "# NOTE: \n",
    "#    - The directory (`CV_DATAROOT_00X`) is assumed to be populated with a holdout of images from MSLS. \n",
    "#    - `DATASET_SIGMA` can be set below to skip the sigma estimation/cross-validation step...\n",
    "CV_DATAROOT_001 = \"/data/cross_validation_images/set001/\" \n",
    "CV_DATAROOT_002 = \"/data/cross_validation_images/set002/\" \n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# If you want to skip sigma validation, use the following as a *ROUGH* value; else \n",
    "# set DATASET_SIGMA == None\n",
    "DATASET_SIGMA = np.logspace(-1., 0, 10)[5]\n",
    "\n",
    "VALIDATION_SAMPLE_SIZE = 5000\n",
    "\n",
    "# Set Estimation Epoch && Number of Samples to Generate\n",
    "ESTIMATION_EPOCH = 16\n",
    "N_SAMPLES = 1000\n",
    "\n",
    "# See `Data and Transformations` section for details.\n",
    "TORCH_DL_COMPOSED_TRANSFORMS = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.3, 0.0)),\n",
    "    transforms.CenterCrop(IMG_SIZE * 4),\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54674f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageFolder/Dataloader reads from the directory of images and applys a transformation\n",
    "# at runtime to generate our training images (tensors) from source images\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    root=CV_DATAROOT_001,\n",
    "    transform=TORCH_DL_COMPOSED_TRANSFORMS\n",
    ")\n",
    "\n",
    "# Use LimitDataset wrapper to ensure we're able to transfer into memory safely\n",
    "limited_msls_data = utils.LimitDataset(\n",
    "    dataset, min(VALIDATION_SAMPLE_SIZE, len(dataset))\n",
    ")\n",
    "\n",
    "# Create the dataloader\n",
    "#\n",
    "# WARNING: By setting `batch_size=len(dataset)`, we're forcing the loader to read all data in a single\n",
    "# iteration. This is \"safe\" ONLY because we've used `utils.LimitDataset` above to fix the amount of\n",
    "# images/memory that operation will use!\n",
    "msls_real_dataloader = torch.utils.data.DataLoader(\n",
    "    limited_msls_data,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    batch_size=len(dataset),  # Force Single Batch\n",
    ")\n",
    "\n",
    "# The implementation of Parzen LL expects data in a particular shape; convert from\n",
    "# [N x 3 x 64 x 64] => [N x (3 x 64 x 64)]\n",
    "msls_real_data = next(iter(msls_real_dataloader))[0].numpy()\n",
    "print(f\"Shape after Fetch From Loader: {msls_real_data.shape}\")\n",
    "\n",
    "msls_real_data = msls_real_data.reshape(\n",
    "    (msls_real_data.shape[0], np.prod(msls_real_data.shape[1:]))\n",
    ")\n",
    "\n",
    "print(f\"Shape after Reshape: {msls_real_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a8ec9e",
   "metadata": {},
   "source": [
    "## Initialize Model and Training Configs \n",
    "\n",
    "Model and training configs are required to specificy the model to load and generate samples from `G` as of a specific epoch\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304dbe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model and Training Configs w. default args. Not strictly required, just for clarity.\n",
    "model_cfg = dcgan.ModelCheckpointConfig(\n",
    "    model_name=\"msls_dcgan_ml_p3_8xlarge_001\",  # Custom Model Name To Identify Gaudi vs GPU Trained!\n",
    "    model_dir=\"/efs/trained_model\",\n",
    "    save_frequency=1,\n",
    "    log_frequency=50,\n",
    "    gen_progress_frequency=250,\n",
    ")\n",
    "\n",
    "train_cfg = dcgan.TrainingConfig(\n",
    "    batch_size=128,\n",
    "    img_size=64,\n",
    "    nc=3,\n",
    "    nz=100,\n",
    "    ngf=64,\n",
    "    ndf=64,\n",
    "    lr=0.0002,\n",
    "    beta1=0.5,\n",
    "    beta2=0.999,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e563ba",
   "metadata": {},
   "source": [
    "## Calculating Log-Likelihood For a Single Epoch\n",
    "\n",
    "In the section below we generate samples from `G` as of a specific epoch, `ESTIMATION_EPOCH`. We then use these samples to calculate a `Sigma` from a set of candidate values, and then estimate the Log-Likelihood of the test set.\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50165af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate N samples\n",
    "generated_data = dcgan.generate_fake_samples(\n",
    "    n_samples=N_SAMPLES,\n",
    "    train_cfg=train_cfg,\n",
    "    model_cfg=model_cfg,\n",
    "    as_of_epoch=ESTIMATION_EPOCH,\n",
    ").numpy()\n",
    "\n",
    "print(f\"Shape after Generation: {generated_data.shape}\")\n",
    "\n",
    "# The implementation of Parzen LL expects data in a particular shape; convert from\n",
    "# [N x 3 x 64 x 64] => [N x (3 x 64 x 64)]\n",
    "\n",
    "generated_data = generated_data.reshape(\n",
    "    (generated_data.shape[0], np.prod(generated_data.shape[1:]))\n",
    ")\n",
    "\n",
    "print(f\"Shape after Reshape: {generated_data.shape}\")\n",
    "\n",
    "# Skip Sigma Calcs if Sigma is already set...\n",
    "if DATASET_SIGMA:\n",
    "    print(f\"Skipping Sigma Estimation, Using: {DATASET_SIGMA}\")\n",
    "    sigma = DATASET_SIGMA\n",
    "else:\n",
    "    # Estimate Sigma on G(Z) and MSLS Data...\n",
    "    sigma = plle.cross_validate_sigma(\n",
    "        generated_data,\n",
    "        msls_real_data,\n",
    "        np.logspace(-1.0, 0, num=10),  # Default Sigma-space from DCGAN\n",
    "        BATCH_SIZE,  # Default Batch Size\n",
    "    )\n",
    "\n",
    "# Fit Parzen Estimator && Calculate LL\n",
    "parzen = plle.theano_parzen(generated_data, sigma)\n",
    "\n",
    "ll = plle.get_nll(msls_real_data, parzen, batch_size=BATCH_SIZE)\n",
    "\n",
    "se = ll.std() / np.sqrt(msls_real_data.shape[0])\n",
    "\n",
    "print(f\"Log-Likelihood of Test Set = {ll.mean()}, se: {se}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e960ace",
   "metadata": {},
   "source": [
    "## Calculate Log-Likelihood For Multiple Epochs\n",
    "\n",
    "Same procedure as above, calculate the LL over multiple epochs. Loading and generating from `G` across multiple\n",
    "checkpoints. Uses a fixed `sigma`. This may not be the optimal method to asses the quality of a GAN's output, but it does demonstrate progress over time.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Parzen Estimator && Calculate LL\n",
    "EPOCH_FREQUENCY = 1\n",
    "PLOT_DTTM = re.sub(\":|-| |\\.\", \"_\", datetime.datetime.utcnow().__str__())\n",
    "\n",
    "log_likelihoods = []\n",
    "std_errs = []\n",
    "\n",
    "# Create new TensorBoard Writer...\n",
    "writer = SummaryWriter(f\"{model_cfg.model_dir}/{model_cfg.model_name}/events\")\n",
    "\n",
    "for cur_epoch in range(0, ESTIMATION_EPOCH, EPOCH_FREQUENCY):\n",
    "\n",
    "    # Generate Data as of Epoch\n",
    "    generated_data = dcgan.generate_fake_samples(\n",
    "        n_samples=N_SAMPLES,\n",
    "        train_cfg=train_cfg,\n",
    "        model_cfg=model_cfg,\n",
    "        as_of_epoch=cur_epoch,\n",
    "    ).numpy()\n",
    "\n",
    "    generated_data = generated_data.reshape(\n",
    "        (generated_data.shape[0], np.prod(generated_data.shape[1:]))\n",
    "    )\n",
    "\n",
    "    parzen = plle.theano_parzen(generated_data, sigma)\n",
    "\n",
    "    # Estimate Log-Likelihood\n",
    "    ll = plle.get_nll(msls_real_data, parzen, batch_size=BATCH_SIZE)\n",
    "\n",
    "    se = ll.std() / np.sqrt(msls_real_data.shape[0])\n",
    "    log_likelihoods.append(ll.mean())\n",
    "    std_errs.append(se)\n",
    "\n",
    "    # Write to TensorBoard...\n",
    "    writer.add_scalar(\n",
    "        f\"parzen_estimated_LL\",\n",
    "        ll.mean(),\n",
    "        cur_epoch,\n",
    "    )\n",
    "    writer.flush()\n",
    "\n",
    "    print(f\"Epoch {cur_epoch}: Log-Likelihood of Test Set = {ll.mean()}, se: {se}\")\n",
    "\n",
    "\n",
    "# Plot LL over Range && Save to TensorBoard\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(f\"Log Likliehood over Training Epochs - {model_cfg.model_name}\")\n",
    "\n",
    "plt.plot(range(0, ESTIMATION_EPOCH, EPOCH_FREQUENCY), lls)\n",
    "\n",
    "plt.errorbar(\n",
    "    range(0, ESTIMATION_EPOCH, EPOCH_FREQUENCY), log_likelihoods, yerr=std_errs, fmt=\"o\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Log-Likelihood\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\n",
    "    f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures/log_likelihood_{PLOT_DTTM}.png\"\n",
    ")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dfe4e9",
   "metadata": {},
   "source": [
    "## Calculating Log Likelihood on Samples from the Test Data\n",
    "\n",
    "> We estimate probability of the test set data under *Pg* by fitting a Gaussian Parzen window to the samples generated with G and reporting the log-likelihood under this distribution.\n",
    "\n",
    "To put these values into context, I'm going to run the procedure against a sample of real images...\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63add51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Duplicate Steps from 2.3\n",
    "\n",
    "# Create Secondary Dataset of Real Images; Treat these as if they were \"fake\"\n",
    "cv_dataset_002 = torchvision.datasets.ImageFolder(\n",
    "    root=CV_DATAROOT_002,\n",
    "    transform=TORCH_DL_COMPOSED_TRANSFORMS\n",
    ")\n",
    "\n",
    "# Use LimitDataset wrapper to ensure we're able to transfer into memory safely\n",
    "limited_msls_data_002 = utils.LimitDataset(\n",
    "    dataset, min(VALIDATION_SAMPLE_SIZE, len(cv_dataset_002))\n",
    ")\n",
    "\n",
    "# Create the Dataloader\n",
    "msls_real_data_batch_002 = torch.utils.data.DataLoader(\n",
    "    limited_msls_data_002,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    batch_size=len(cv_dataset_002),  # Force Single Batch\n",
    ")\n",
    "\n",
    "# Convert from [N x 3 x 64 x 64] => [N x (3 x 64 x 64)]\n",
    "msls_real_data_batch_002 = next(iter(msls_real_data_batch_002))[0].numpy()\n",
    "\n",
    "msls_real_data_batch_002 = msls_real_data.reshape(\n",
    "    (msls_real_data_batch_002.shape[0], np.prod(msls_real_data_batch_002.shape[1:]))\n",
    ")\n",
    "\n",
    "# Fit Parzen Estimator && Calculate LL on Two Batches of Real Data\n",
    "parzen = plle.theano_parzen(msls_real_data_batch_002, sigma)\n",
    "\n",
    "ll = plle.get_nll(msls_real_data_batch_001, parzen, batch_size=BATCH_SIZE)\n",
    "\n",
    "se = ll.std() / np.sqrt(msls_real_data_batch_001.shape[0])\n",
    "\n",
    "print(f\"Log-Likelihood of Test Set = {ll.mean()}, se: {se}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
