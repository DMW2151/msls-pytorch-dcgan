{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897d036b",
   "metadata": {},
   "source": [
    "# DCGAN - Supplemental Media and Figures\n",
    "\n",
    "Contains supplemental media and figures, currently includes the following:\n",
    "\n",
    "- Training Progress Video \n",
    "- Training Progress Gif\n",
    "- Training Loss\n",
    "- CPU Generated Samples\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e36995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ffmpeg - Probably Not on Kernel by Default\n",
    "!pip3 install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b13656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Deps\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Plotting + Video\n",
    "import ffmpeg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# DCGAN\n",
    "import gaudi_dcgan as dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Start Datetime for plots; apply all the way through the Notebook\n",
    "PLOT_DTTM = re.sub(\":|-| |\\.\", \"_\", datetime.datetime.utcnow().__str__())\n",
    "EPOCH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df14244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Model and Training Configs w. Default Values - See gaudi_dcgan.py for descriptions. For clarity,\n",
    "# objects below are initialized with their default values.\n",
    "model_cfg = dcgan.ModelCheckpointConfig(\n",
    "    model_name=\"msls_dcgan_ml_p3_8xlarge_001\", # Custom Model Name To Identify Gaudi vs GPU trained\n",
    "    model_dir=\"/efs/trained_model\",\n",
    "    save_frequency=1,\n",
    "    log_frequency=50,\n",
    "    gen_progress_frequency=250,\n",
    ")\n",
    "\n",
    "train_cfg = dcgan.TrainingConfig(\n",
    "    batch_size=128,\n",
    "    img_size=64,\n",
    "    nc=3,\n",
    "    nz=100,\n",
    "    ngf=64,\n",
    "    ndf=64,\n",
    "    lr=0.0002,\n",
    "    beta1=0.5,\n",
    "    beta2=0.999,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Figures and Videos Directory if Not Yet Exists...\n",
    "if not os.path.exists(f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures\"):\n",
    "    os.makedirs(f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures\")\n",
    "\n",
    "if not os.path.exists(f\"{model_cfg.model_dir}/{model_cfg.model_name}/videos\"):\n",
    "    os.makedirs(f\"{model_cfg.model_dir}/{model_cfg.model_name}/videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c85dc",
   "metadata": {},
   "source": [
    "## Get Model Checkpoint For Evaluation\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b24f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Net and Optimizers\n",
    "netD, optimD = train_cfg.get_net_D()\n",
    "netG, optimG = train_cfg.get_net_G()\n",
    "\n",
    "# Check the save-path for a model with this name && Load Params\n",
    "cur_epoch, losses, fixed_noise, img_list = dcgan.instantiate_from_checkpoint(\n",
    "    netD, netG, optimD, optimG, f\"{model_cfg.model_dir}/{model_cfg.model_name}/checkpoint_{EPOCH}.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f13e73",
   "metadata": {},
   "source": [
    "## Figure 1.1 - `G` and `D` Training Losses\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Training Losses to the Generator (G) and Discriminator (D)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(f\"Generator and Discriminator Loss During Training - {model_cfg.model_name}\")\n",
    "plt.plot(losses[\"_G\"], label=\"G\")\n",
    "plt.plot(losses[\"_D\"], label=\"D\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\n",
    "    f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures/train_loss_{PLOT_DTTM}.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f4087",
   "metadata": {},
   "source": [
    "## Figure 2.1.1 - Final Images of Fixed Noise Sample vs Real Images\n",
    "\n",
    "Get a batch of real images from the dataloader and compare the final generated images vs. the real images. Do they hold up against human discretion? Note that this figure uses fixed noise saved as part of the model checkpoint and will **NOT** generate new images from `G` on subsequent runs.\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5832c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In general, the ImageFolder/Dataloader reads from the directory of images and applys a transformation\n",
    "# at runtime to generate our training images. See `Data and Transformations` section for details.\n",
    "dataset = dset.ImageFolder(\n",
    "    root=\"/efs/sample_images\",\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.3, 0.0)),\n",
    "            transforms.CenterCrop(train_cfg.img_size * 4),\n",
    "            transforms.Resize(train_cfg.img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                ),\n",
    "                (\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# NOTE: This step can be slow as the images are processed (esp. on a new EFS); anecdotally, around 8-10 \n",
    "# min to load 1MM images (~30GB total)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    timeout=0,\n",
    "    batch_size=train_cfg.batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images from the dataloader \n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        vutils.make_grid(\n",
    "            real_batch[0].to(train_cfg.dev)[:64], padding=5, normalize=True\n",
    "        ).cpu(),\n",
    "        (1, 2, 0),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the fake images from the final epoch of `G`\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images\")\n",
    "plt.imshow(np.transpose(img_list[-1], (1, 2, 0)))\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\n",
    "    f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures/compare_{PLOT_DTTM}.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6448d3",
   "metadata": {},
   "source": [
    "## Figure 2.1.2 - Training Progress Sequence on Fixed Noise\n",
    "\n",
    "In each epoch (or at some fixed interval, e.g. every other, every third) during training the model saved the progress of `G` on transforming a series of fixed inputs, `Z`. This figure shows the progress of `Z` as the model trained.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3644ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.2 - Training Progress Sequence Saved as video && GIF\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=150, blit=True)\n",
    "\n",
    "writergif = animation.PillowWriter(\n",
    "    fps=10, metadata=dict({\"title\": f\"{model_cfg.model_name}\"}, **model_cfg.__dict__)\n",
    ")\n",
    "\n",
    "# Save GIF\n",
    "ani.save(\n",
    "    f\"{model_cfg.model_dir}/{model_cfg.model_name}/videos/progress_{PLOT_DTTM}.gif\",\n",
    "    writer=writergif,\n",
    ")\n",
    "\n",
    "# Save Video\n",
    "with open(\n",
    "    f\"{model_cfg.model_dir}/{model_cfg.model_name}/videos/progress_{PLOT_DTTM}.html\", \"w\",\n",
    ") as fi:\n",
    "    print(ani.to_html5_video(), file=fi)\n",
    "\n",
    "# Show Video - Note; this can get quite large if there are too many frames or too many fixed\n",
    "# noise images saved along with the model's progress checkpoints...\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d447fcf2",
   "metadata": {},
   "source": [
    "## Figure 3.1 - Generate New Samples From CPU\n",
    "\n",
    "This figure generates novel images from `G` by creating a new input vector `Z` and feeding it through the network. Because `Z` is regenerated on each run, these results will vary between executions.\n",
    "\n",
    "Being able to run \"inference\" quickly is important. Ideally `G` can generate samples using just the CPU. Using `G` to generate samples on the CPU allows us to serve new imaages off an inexpensive instance.\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3.1 - Generate New Samples From CPU\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Create a new TrainingConfig which will generate noise on the CPU\n",
    "train_cfg_cpu_only = dcgan.TrainingConfig(dev=torch.device(\"cpu\"))\n",
    "\n",
    "# Generate Samples from `G` (again, on CPU)\n",
    "imgs = dcgan.generate_fake_samples(\n",
    "    n_samples=4, train_cfg=train_cfg_cpu_only, model_cfg=model_cfg, as_of_epoch=EPOCH\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        vutils.make_grid(imgs.to(train_cfg.dev), padding=2, normalize=True).cpu(),\n",
    "        (1, 2, 0),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save Result\n",
    "plt.savefig(\n",
    "    f\"{model_cfg.model_dir}/{model_cfg.model_name}/figures/novel_samples_{PLOT_DTTM}.png\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
