{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3221c15",
   "metadata": {},
   "source": [
    "# DCGAN - Parzen Window-based Log-Likelihood Estimates\n",
    "\n",
    "This notebook is a wrapper around the Parzen log-liklihood estimator described and implemented\n",
    "in the [original DCGAN paper](https://github.com/goodfeli/adversarial/blob/master/parzen_ll.py). \n",
    "\n",
    "> We estimate probability of the test set data under pg by fitting a Gaussian Parzen window to the\n",
    "samples generated with G and reporting the log-likelihood under this distribution. The σ parameter of the Gaussians was obtained by cross validation on the validation set. This procedure was intro-\n",
    "duced in Breuleux et al. [8] and used for various generative models for which the exact likelihood\n",
    "is not tractable\n",
    "\n",
    "Slight modifications are made in the local file (`parzen_ll.py`) for the following:\n",
    "\n",
    "- Migrate from Python2 -> Python3 syntax\n",
    "- Add comments and docstrings for clarity\n",
    "\n",
    "The goal of this project is not to develop a new  ramework for estimating generative models, consequently, the log-likelihoods calculated here are meant only for internal comparison between models. As you'll notice, I do not use `MNIST`, `TFD`, or `CIFAR-10` as a validation set, but rather a sample of MSLS images held-out from training.\n",
    "\n",
    "**Note:** Only tested on `conda_amazonei_pytorch_latest_p3X` and `python_latest_p3X`\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# These are NOT on all `conda_amazonei_pytorch_latest_p3X` or `conda_pytorch_p3X` builds\n",
    "! pip3 install ./../model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "# Torch Deps\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# DCGAN\n",
    "import msls.gpu_dcgan as dcgan\n",
    "import msls.dcgan_utils as utils\n",
    "import msls.gan as gan\n",
    "import msls.evaluation as evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ef253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs \n",
    "# NOTE: \n",
    "#    - The directory (`CV_DATAROOT_00X`) is assumed to be populated with a holdout of images from MSLS. \n",
    "\n",
    "# Normally these don't need to be set; this is a special case to allow for reshaping for cross-validation\n",
    "# w. parzen est.\n",
    "CV_DATAROOT = \"/efs/imgs/test/miami\" \n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SAMPLE_SIZE = 100\n",
    "EPOCH_FREQUENCY = 1\n",
    "ESTIMATION_EPOCH = 16\n",
    "N_SAMPLES = 100\n",
    "\n",
    "# See `Data and Transformations` section for details.\n",
    "TORCH_DL_COMPOSED_TRANSFORMS = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.3, 0.0)),\n",
    "    transforms.CenterCrop(IMG_SIZE * 4),\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54674f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageFolder/Dataloader reads from the directory of images and applys a transformation\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    root=CV_DATAROOT,\n",
    "    transform=TORCH_DL_COMPOSED_TRANSFORMS\n",
    ")\n",
    "\n",
    "# Use LimitDataset wrapper to ensure we're able to transfer into memory safely\n",
    "# WARNING: By setting `batch_size=len(dataset)`, we're forcing the loader to read all data in a single\n",
    "# iteration. This is \"safe\" ONLY because we've used `utils.LimitDataset` above to fix the amount of\n",
    "# images/memory that operation will use!\n",
    "limited_msls_data = utils.LimitDataset(\n",
    "    dataset, min(VALIDATION_SAMPLE_SIZE, len(dataset))\n",
    ")\n",
    "\n",
    "msls_real_dataloader = torch.utils.data.DataLoader(\n",
    "    limited_msls_data,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    batch_size=len(limited_msls_data),  # Force Single Batch\n",
    ")\n",
    "\n",
    "# The implementation of Parzen LL expects data in a particular shape; convert from\n",
    "# [N x 3 x 64 x 64] => [N x (3 x 64 x 64)]\n",
    "msls_real_data = next(iter(msls_real_dataloader))[0].numpy()\n",
    "print(f\"Shape after Fetch From Loader: {msls_real_data.shape}\")\n",
    "\n",
    "msls_real_data = msls_real_data.reshape(\n",
    "    (msls_real_data.shape[0], np.prod(msls_real_data.shape[1:]))\n",
    ")\n",
    "\n",
    "print(f\"Shape after Reshape: {msls_real_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a8ec9e",
   "metadata": {},
   "source": [
    "## Initialize Model and Training Configs \n",
    "\n",
    "Model and training configs are required to specificy the model to load and generate samples from `G` as of a specific epoch\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304dbe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model and Training Configs w. default args. \n",
    "\n",
    "model_cfg = dcgan.ModelCheckpointConfig(\n",
    "    name=\"msls-dcgan-128\",  # Custom Model Name To Identify Gaudi vs GPU Trained!\n",
    "    root=\"/efs/trained_model\",\n",
    "    save_frequency=1,\n",
    "    log_frequency=50,\n",
    ")\n",
    "\n",
    "\n",
    "# Train Config: Must Have Same Size Params as Model...\n",
    "# python3 -m msls.run_dcgan \\\n",
    "#     -c '{\"name\": \"msls-dcgan-128\", \"root\": \"/efs/trained_model/\", \"log_frequency\": 50, \"save_frequency\": 1}' \\\n",
    "#     -t '{\"nc\": 3, \"nz\": 256, \"ngf\": 256, \"ndf\": 64, \"lr\": 0.0002, \"beta1\": 0.5, \"beta2\": 0.999, \"batch_size\": 256, \"img_size\": 64, \"weight_decay\": 0.05}'\\\n",
    "#     --s_epoch 0 \\\n",
    "#     --n_epoch 16 \\\n",
    "#     --dataroot /data/imgs/train_val/helsinki \\\n",
    "#     --logging True \\\n",
    "#     --profile True  \\\n",
    "#     --s3_bucket 'dmw2151-habana-model-outputs'\n",
    "\n",
    "train_cfg = dcgan.TrainingConfig(\n",
    "    dev = torch.device(\"cpu\"),\n",
    "    data_root = CV_DATAROOT,\n",
    "    nz = 256,\n",
    "    nc = 3,\n",
    "    ngf = 256,\n",
    "    ndf = 64,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e563ba",
   "metadata": {},
   "source": [
    "## Calculating Log-Likelihood For a Single Epoch\n",
    "\n",
    "In the section below we generate samples from `G` as of a specific epoch, `ESTIMATION_EPOCH`. We then use these samples to calculate a `Sigma` from a set of candidate values, and then estimate the Log-Likelihood of the test set.\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cabb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restore the Generator for Creating New Images\n",
    "G, opt_G = train_cfg.get_network(gan.Generator64, device_rank=0)\n",
    "\n",
    "checkpoint = utils.get_checkpoint(\n",
    "    path=f\"{model_cfg.root}/{model_cfg.name}/checkpoint_{ESTIMATION_EPOCH}.pt\",\n",
    "    cpu=True,\n",
    ")\n",
    "\n",
    "utils.restore_G_for_inference(checkpoint, G)\n",
    "\n",
    "# Generate N samples\n",
    "Z = torch.randn(N_SAMPLES, train_cfg.nz, 1, 1, device=train_cfg.dev)\n",
    "generated_data = G(Z).detach().numpy()\n",
    "\n",
    "# The implementation of Parzen LL expects data in a particular shape; convert from\n",
    "# [N x 3 x 64 x 64] => [N x (3 x 64 x 64)]\n",
    "generated_data = generated_data.reshape(\n",
    "    (generated_data.shape[0], np.prod(generated_data.shape[1:]))\n",
    ")\n",
    "\n",
    "# Estimate Sigma on G(Z) and MSLS Data...\n",
    "sigma = evaluation.cross_validate_sigma(\n",
    "    generated_data,\n",
    "    msls_real_data,\n",
    "    np.logspace(-1.0, 0, num=10),  # Default Sigma-space from DCGAN\n",
    "    BATCH_SIZE,  # Default Batch Size\n",
    ")\n",
    "\n",
    "# Fit Parzen Estimator && Calculate LL\n",
    "parzen = evaluation.theano_parzen(generated_data, sigma)\n",
    "\n",
    "ll = evaluation.get_nll(msls_real_data, parzen, batch_size=BATCH_SIZE)\n",
    "\n",
    "se = ll.std() / np.sqrt(msls_real_data.shape[0])\n",
    "\n",
    "print(f\"Log-Likelihood Results:\\t[σ : {sigma:.4f}]\\t[nll: {ll.mean():.4f}]\\t[se: {se:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e960ace",
   "metadata": {},
   "source": [
    "## Calculate Log-Likelihood For Multiple Epochs\n",
    "\n",
    "Same procedure as above, calculate the LL over multiple epochs. Loading and generating from `G` across multiple\n",
    "checkpoints. Uses a fixed `sigma`. This may not be the optimal method to asses the quality of a GAN's output, but it does demonstrate progress over time.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Parzen Estimator && Calculate LL\n",
    "log_likelihoods = []\n",
    "std_errs = []\n",
    "\n",
    "# Generate N samples\n",
    "Z = torch.randn(N_SAMPLES, train_cfg.nz, 1, 1, device=train_cfg.dev)\n",
    "\n",
    "for cur_epoch in range(0, ESTIMATION_EPOCH, EPOCH_FREQUENCY):\n",
    "    \n",
    "    # Restore G to a Particular Epoch\n",
    "    checkpoint = utils.get_checkpoint(\n",
    "        path=f\"{model_cfg.root}/{model_cfg.name}/checkpoint_{cur_epoch}.pt\",\n",
    "        cpu=True,\n",
    "    )\n",
    "    \n",
    "    utils.restore_G_for_inference(checkpoint, G)\n",
    "    \n",
    "    # Generate Data as of Epoch\n",
    "    generated_data = G(Z).detach().numpy()\n",
    "\n",
    "    generated_data = generated_data.reshape(\n",
    "        (generated_data.shape[0], np.prod(generated_data.shape[1:]))\n",
    "    )\n",
    "\n",
    "    parzen = evaluation.theano_parzen(generated_data, sigma)\n",
    "\n",
    "    # Estimate Log-Likelihood\n",
    "    ll = evaluation.get_nll(msls_real_data, parzen, batch_size=BATCH_SIZE)\n",
    "\n",
    "    se = ll.std() / np.sqrt(msls_real_data.shape[0])\n",
    "    log_likelihoods.append(ll.mean())\n",
    "    std_errs.append(se)\n",
    "    \n",
    "    print(f\"[Epoch: {cur_epoch}]\\t[σ : {sigma:.4f}]\\t[nll: {ll.mean():.4f}]\\t[se: {se:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbee92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LL over Range && Save to TensorBoard\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(f\"Log Likliehood over Training Epochs - {model_cfg.name}\")\n",
    "\n",
    "plt.plot(range(0, ESTIMATION_EPOCH, EPOCH_FREQUENCY), log_likelihoods)\n",
    "\n",
    "plt.errorbar(\n",
    "    range(0, ESTIMATION_EPOCH, EPOCH_FREQUENCY), log_likelihoods, yerr=std_errs, fmt=\"o\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Log-Likelihood\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\n",
    "    f\"{model_cfg.root}/{model_cfg.name}/figures/log_likelihood.png\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
