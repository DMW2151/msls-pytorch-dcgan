<!doctype html>
<html lang="en">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon"> 
    <meta charset="utf-8">
    <meta name="author" content='Dustin Wilson'>
    <meta name="date" content='2022-01-29'>
    <title>Generative Street-Level Imagery on DL1 Instances</title>
    <style>
            html {
              line-height: 1.5;
              font-family: Georgia, serif;
              font-size: 16px;
              color: #1a1a1a;
              background-color: #202025;
            }
            body {
              margin: 0 auto;
              max-width: 54em;
              padding-left: 50px;
              padding-right: 50px;
              padding-top: 50px;
              padding-bottom: 50px;
              hyphens: auto;
              overflow-wrap: break-word;
              text-rendering: optimizeLegibility;
              font-kerning: normal;
            }
            @media (max-width: 600px) {
              body {
                font-size: 0.9em;
                padding: 1em;
              }
            }
            @media print {
              body {
                background-color: transparent;
                color: black;
                font-size: 12pt;
              }
              p, h2, h3 {
                orphans: 3;
                widows: 3;
              }
              h2, h3, h4 {
                page-break-after: avoid;
              }
            }
            p {
              margin: 1em 0;
            }
            a {
              color: #5858d9;
            }
            a:visited {
              color: #9292f2;
            }
            img {
              max-width: 100%;
            }
            h1, h2, h3, h4, h5, h6 {
              margin-top: 1.4em;
            }
            h5, h6 {
              font-size: 1em;
              font-style: italic;
            }
            h6 {
              font-weight: normal;
            }
            ol, ul {
              padding-left: 1.7em;
              margin-top: 1em;
            }
            li > ol, li > ul {
              margin-top: 0;
            }
            blockquote {
              margin: 1em 0 1em 1.7em;
              padding-left: 1em;
              border-left: 2px solid #e6e6e6;
              color: #606060;
            }
            code {
              font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
              font-size: 85%;
              margin: 0;
              color: #FFFBC8;
            }
            pre {
              margin: 1em 0;
              overflow: auto;
            }
            pre code {
              padding: 0;
              overflow: visible;
              overflow-wrap: normal;
              color: #ebebe6;
            }
            .sourceCode {
             background-color: transparent;
             overflow: visible;
            }
            hr {
              background-color: #ebebe6;
              border: none;
              height: 1px;
              margin: 1em 0;
            }
            table {
              margin: 1em 0;
              border-collapse: collapse;
              width: 100%;
              overflow-x: auto;
              display: block;
              font-variant-numeric: lining-nums tabular-nums;
              color: #ebebe6;
            }
            table caption {
              margin-bottom: 0.75em;
            }
            tbody {
              margin-top: 0.5em;
              border-top: 1px solid #202025;
              border-bottom: 1px solid #202025;
            }
            th {
              border-top: 1px solid #202025;
              padding: 0.25em 0.5em 0.25em 0.5em;
            }
            td {
              padding: 0.125em 0.5em 0.25em 0.5em;
              color: #ebebe6;
            }
            header {
              margin-bottom: 4em;
              text-align: center;
            }
            #TOC li {
              list-style: none;
            }
            #TOC a:not(:hover) {
              text-decoration: none;
            }
            code{white-space: pre-wrap;}
            span.smallcaps{font-variant: small-caps;}
            span.underline{text-decoration: underline;}
            div.column{display: inline-block; vertical-align: top; width: 50%;}
            div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
            ul.task-list{list-style: none;}
            pre > code.sourceCode { white-space: pre; position: relative; }
            pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
            pre > code.sourceCode > span:empty { height: 1.2em; }
            .sourceCode { overflow: visible; }
            code.sourceCode > span { color: inherit; text-decoration: inherit; }
            div.sourceCode { margin: 1em 0; }
            pre.sourceCode { margin: 0; }
            @media screen {
            div.sourceCode { overflow: auto; }
            }
            @media print {
            pre > code.sourceCode { white-space: pre-wrap; }
            pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
            }
            pre.numberSource code
              { counter-reset: source-line 0; }
            pre.numberSource code > span
              { position: relative; left: -4em; counter-increment: source-line; }
            pre.numberSource code > span > a:first-child::before
              { content: counter(source-line);
                position: relative; left: -1em; text-align: right; vertical-align: baseline;
                border: none; display: inline-block;
                -webkit-touch-callout: none; -webkit-user-select: none;
                -khtml-user-select: none; -moz-user-select: none;
                -ms-user-select: none; user-select: none;
                padding: 0 4px; width: 4em;
                color: #e6e6e6;
              }
            pre.numberSource { margin-left: 3em; border-left: 1px solid #e6e6e6;  padding-left: 4px; }
            div.sourceCode
              {   }
            @media screen {
            pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
            }
            code span.al { color: #ff0000; font-weight: bold; } /* Alert */
            code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
            code span.at { color: #FFFBC8; } /* Attribute */
            code span.bn { color: #FFFBC8; } /* BaseN */
            code span.bu { } /* BuiltIn */
            code span.cf { color: #FFFBC8; font-weight: bold; } /* ControlFlow */
            code span.ch { color: #4070a0; } /* Char */
            code span.cn { color: #880000; } /* Constant */
            code span.co { color: #60a0b0; font-style: italic; } /* Comment */
            code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
            code span.do { color: #ba2121; font-style: italic; } /* Documentation */
            code span.dt { color: #902000; } /* DataType */
            code span.dv { color: #FFFBC8; } /* DecVal */
            code span.er { color: #ff0000; font-weight: bold; } /* Error */
            code span.ex { } /* Extension */
            code span.fl { color: #FFFBC8; } /* Float */
            code span.fu { color: #06287e; } /* Function */
            code span.im { } /* Import */
            code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
            code span.kw { color: #FFFBC8; font-weight: bold; } /* Keyword */
            code span.op { color: #666666; } /* Operator */
            code span.ot { color: #FFFBC8; } /* Other */
            code span.pp { color: #bc7a00; } /* Preprocessor */
            code span.sc { color: #4070a0; } /* SpecialChar */
            code span.ss { color: #bb6688; } /* SpecialString */
            code span.st { color: #4070a0; } /* String */
            code span.va { color: #19177c; } /* Variable */
            code span.vs { color: #4070a0; } /* VerbatimString */
            code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
            .display.math{display: block; text-align: center; margin: 0.5rem auto;}
          </style>
  </head>
  <body style="background-color:#202025; color: #e6e6e6"></body>
    <h1> Generative Street-Level Imagery on DL1 Instances </h1>
    <p>Dustin Wilson &#8212 January 29, 2022</p>
    <hr />
    <center>
    <figure class="image">
    <img src="./images/gan/001.gif" height="auto" width="188" style="padding: 20px; border-radius: 2px"> <img src="./images/gan/002.gif" height="auto" width="188" style="padding: 20px; border-radius: 2px"> <img src="./images/gan/003.gif" height="auto" width="188" style="padding: 20px; border-radius: 2px"> <i>
    <figcaption style="font-size: 12px;">
    Nowhere, USA - Experimental Output - Scenes created by interpolating between sequences of generated frames
    </figcaption>
    </i>
    </figure>
    </center>
    <p>This post is a companion to my entry in the <a href="https://amazon-ec2-dl1.devpost.com">AWS Deep Learning Challenge</a>. The event encouraged participants to use AWS’ DL1 instances to scale deep learning model training. I’m not much of an ML engineer, but this event offered a perfect opportunity to implement a model, instrument my code to train on <a href="https://habana.ai/wp-content/uploads/2019/06/Habana-Gaudi-Training-Platform-whitepaper.pdf">Gaudi accelerators</a>, and then perform a comparative analysis of performance across training environments. At a high level, my project involved re-implementing elements of foundational papers in the study of generative computer vision and then training a model on over 1.5 million street-level images.</p>
    <ul>
    <li><a href="#Theory-and-Background">Theory and Background</a></li>
    <li><a href="#Mapillary-Street-Level-Imagery-Data">Mapillary Street Level Imagery Data</a></li>
    <li><a href="#AWS-System-Architecture">AWS System Architecture</a></li>
    <li><a href="#Modifications-for-Training-on-Gaudi-Accelerated-Instances-DL1">Modifications for Training on Gaudi Accelerated Instances (<code>DL1</code>)</a></li>
    <li><a href="#DCGAN-Results">DCGAN Results</a></li>
    <li><a href="#Comparative-Performance">Comparative Performance</a></li>
    <li><a href="#Appendix-1---Comparable-Instance-Selection">Appendix 1 - Comparable Instance Selection</a></li>
    <li><a href="#Citations">Citations</a></li>
    </ul>
    <hr />
    <h3 id="theory-and-background">Theory and Background</h3>
    <p>In this project I re-implement elements of Ian Goodfellow’s <a href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">Generative Adversarial Networks (2014)</a><sup>1</sup> and Alec Radford’s <a href="https://arxiv.org/pdf/1511.06434.pdf">Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks (2016)</a><sup>2</sup> papers in PyTorch. Both papers are concerned with the development of GANs, Generative Adversarial Networks.</p>
    <p>Before discussing specific elements of the project, let’s discuss the <em>way</em> GANs work. Put simply, GANs consist of two competing functions. A generator (<code>G</code>) tries to create believable data and a discriminator (<code>D</code>) tries to maximize the probability it correctly classifies real and generated data.</p>
    <p><strong>Assume the following variables:</strong></p>
    <ul>
    <li><p><code>X</code> — Input data, in our case, an image with size <code>(3 x 64 x 64)</code></p></li>
    <li><p><code>D(X)</code> or <code>D</code> — Discriminator network which outputs the probability that an input, <code>X</code>, is real.</p></li>
    <li><p><code>G(Z)</code> or <code>G</code> — Generator network that deterministically creates data in the shape of <code>X</code>. In practice, an image with size <code>(3 x 64 x 64)</code>.</p></li>
    <li><p><code>Z</code> — Random noise to seed the generator. In practice, a <code>(1 x 100)</code> vector drawn from a standard normal distribution.</p></li>
    <li><p><code>D(G(Z))</code> — Given an output of the generator, the probability that the discriminator believes the image to be real. A high <code>D(G(Z))</code> suggests the generator has “tricked” the discriminator.</p></li>
    </ul>
    <p>The critical steps in each training iteration involve measuring the values of the following terms. For the formula-inclined, the GAN is simply maximizing the following function:</p>
    <center>
    <code>min​</code><sub><code>G</code></sub><code>max​</code><sub><code>V</code></sub><code>(D,G) = E</code><sub><code>x∼pdata​(x)</code></sub>​<code>[logD(x)] + E</code><sub><code>z∼pz​(z)​</code></sub><code>[log(1−D(G(z)))]</code>
    </center>
    <ul>
    <li><p><code>E</code><sub><code>x∼pdata​(x)</code></sub>​<code>[logD(x)]</code> — The expected value of <code>D</code>’s predictions when given samples from the real batch. Remember, <code>D(x)</code> produces a probability, thus a perfect discriminator would return values near <em>0</em>.</p></li>
    <li><p><code>E</code><sub><code>z∼pz​(z)​</code></sub><code>[log(1−D(G(z)))]</code> — The expected value of <code>D</code>’s prediction when given samples produced from <code>G(Z)</code>, Because all images in this batch are fake, a better discriminator would predict a lower <code>D(G(Z))</code>, also returning values near <em>0</em>.</p></li>
    </ul>
    <p>In the DCGAN paper, the method by which this function is maximized is by putting batches of images through <code>D</code> and <code>G</code>, where both are convolutional neural networks with a specific layer structure.</p>
    <center>
    <figure>
    <img style="padding-top: 20px;" align="center" width="600" src="./images/translation/gan.png"> <i>
    <figcaption style="font-size: 12px;">
    DBGAN Architecture - As diagramed by Radford, et. al <sup>4<sup>
    </figcaption>
    </i>
    <figure>
    </center>
    <p>At a low-level, it’s difficult to describe all of the internal consequences of using <code>PyTorch</code> rather than the specific packages the authors used. At a high level, I made the following notable changes:</p>
    <ul>
    <li><p>Choose <code>AdamW</code>/<code>FusedAdamW</code> as an optimizer function over <code>SGD</code>. <em>Goodfellow, et al.</em> use a custom <code>SGD</code> <a href="https://github.com/goodfeli/adversarial/blob/master/sgd.py">implementation</a> that is a patched version of pylearn2’s <code>SGD</code>. Instead, I elected for a built-in PyTorch optimizer, <code>AdamW</code>. As an added benefit, Habana offers their own <code>FusedAdamW</code> implementation that should perform quite well on the Gaudi instances.</p></li>
    <li><p>In <em>Goodfellow, et al.</em>, the authors use the procedure described below to estimate the relative performance of multiple generative methods. Rather than using this procedure to evaluate other models, I implemented it for comparing intra-model progress across epochs, see <a href="#DCGAN-Results">results</a> for a deeper discussion of model validation:</p>
    <blockquote>
    <p>We estimate probability of the test set data under Pg by fitting a Gaussian Parzen window to the samples generated with G and reporting the log-likelihood under this distribution. The σ parameter of the Gaussians was obtained by cross validation on the validation set. This procedure was introduced in Breuleux et al. [7] and used for various generative models for which the exact likelihood is not tractable.</p>
    </blockquote></li>
    <li><p>I remove the final <code>Sigmoid</code> layer from <code>D</code>. Typically a binary classification problem like the one <code>D</code> solves would use <a href="https://en.wikipedia.org/wiki/Cross_entropy">Binary Cross Entropy Loss</a> (<code>BCELoss</code>). The way that PyTorch optimizes for mixed-precision operations required I switch to <code>BCEWithLogitLoss</code>, a loss function that expects logits (<code>L∈(−∞,∞)</code>) rather than probabilities (<code>p∈[0,1]</code>). In effect, this change moves the <code>Sigmoid</code> from the network to part of the loss function. Because I only see ~15% improvement in training time using mixed-precision training, I may revert back to the original architecture.</p></li>
    </ul>
    <hr />
    <h3 id="mapillary-street-level-imagery-data">Mapillary Street Level Imagery Data</h3>
    <center>
    <figure>
    <img alt="training_samples_eu" style="padding-top: 20px;" align="center" width="600" src="./images/translation/train_samples_eu.png"> <i>
    <figcaption style="font-size: 12px;">
    Training Samples From MSLS - Cropped and Transformed
    </figcaption>
    </i>
    <figure>
    </center>
    <p>Throughout this project, I used Mapillary’s Street-Level Sequences data (MSLS). Mapillary provides a platform for crowd-sourced maps and street-level imagery, and publishes computer vision research using data collected from this platform. Mapillary has made this and other data publicly available for <a href="https://www.mapillary.com/dataset/places">download</a>. In total, MSLS contains 1.6 million images from 30 major cities on six-continents and covers different seasons, weather, daylight conditions, structural settings, etc.</p>
    <p>The model presented here was trained on a sample of ~940,000 images. The remaining images were reserved for hyperparameter tuning, cross-validation, model evaluation, etc. The figure below shows an estimated count of images included in model training.</p>
    <div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training Sample By Metro Area</span></span>
    <span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">|</span> <span class="ex">Amman</span>     <span class="kw">|</span>  <span class="ex">1,811</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">London</span>    <span class="kw">|</span>   <span class="ex">5,983</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Toronto</span>   <span class="kw">|</span>  <span class="ex">12,802</span> <span class="kw">|</span></span>
    <span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">|</span> <span class="ex">Amsterdam</span> <span class="kw">|</span>  <span class="ex">7,908</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Manila</span>    <span class="kw">|</span>   <span class="ex">5,378</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Trondheim</span> <span class="kw">|</span>   <span class="ex">9,154</span> <span class="kw">|</span></span>
    <span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">|</span> <span class="ex">Austin</span>    <span class="kw">|</span> <span class="ex">28,462</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Melbourne</span> <span class="kw">|</span> <span class="ex">189,945</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Kampala</span>   <span class="kw">|</span>   <span class="ex">2,069</span> <span class="kw">|</span></span>
    <span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">|</span> <span class="ex">Bangkok</span>   <span class="kw">|</span> <span class="ex">40,125</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Moscow</span>    <span class="kw">|</span> <span class="ex">171,878</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Paris</span>     <span class="kw">|</span>   <span class="ex">9,503</span> <span class="kw">|</span></span>
    <span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">|</span> <span class="ex">Boston</span>    <span class="kw">|</span> <span class="ex">14,037</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Nairobi</span>   <span class="kw">|</span>     <span class="ex">887</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Phoenix</span>   <span class="kw">|</span> <span class="ex">156,477</span> <span class="kw">|</span></span>
    <span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">|</span> <span class="ex">Goa</span>       <span class="kw">|</span>  <span class="ex">5,735</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">SF</span>        <span class="kw">|</span>   <span class="ex">4,525</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Zurich</span>    <span class="kw">|</span>   <span class="ex">2,991</span> <span class="kw">|</span></span>
    <span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="kw">|</span> <span class="ex">Budapest</span>  <span class="kw">|</span> <span class="ex">45,800</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Ottawa</span>    <span class="kw">|</span> <span class="ex">123,296</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Sao</span> Paulo <span class="kw">|</span>  <span class="ex">54,098</span> <span class="kw">|</span></span>
    <span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="kw">|</span> <span class="ex">Helsinki</span>  <span class="kw">|</span> <span class="ex">15,228</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Tokyo</span>     <span class="kw">|</span>  <span class="ex">34,836</span> <span class="kw">|</span>     <span class="kw">|</span> <span class="ex">Total</span>     <span class="kw">|</span> <span class="ex">942,928</span> <span class="kw">|</span></span></code></pre></div>
    <p>Because the authors who developed MSLS for their <a href="https://research.mapillary.com/publication/cvpr20c">research</a><sup>3</sup> were specifically interested in place-recognition, the data is organized such that images of the same physical location appear multiple times under different conditions. The images from these sequences are very highly correlated and reduce the diversity of the training set far more than a single repeated image.</p>
    <p>Originally, I was hoping to train a suite of metro-area models, but the effect of individual sequences was too pronounced and the model often reproduced images from the training set. I did some custom filtering to reduce the contribution of individual sequences, but found the most effective strategy was simply adding more metropolitan areas and converging on a single model.</p>
    <p>The effect of multi-image sequences was further reduced by applying random transformations on each image. MSLS contains images up to <code>(3 x 640 x 480)</code>. Because the original DCGAN paper expects <code>(3 x 64 x 64)</code> images, I had leeway to apply cropping, down-scaling, and horizontal translations to all images before passing them through the model. Given the large image shown below, the model could receive any of the variations presented on the right.</p>
    <center>
    <figure>
    <img alt="nyc_sample_imgs" style="padding-top: 20px;" align="center" width="600" src="./images/translation/nyc_img_transformed_samples.png"> <i>
    <figcaption style="font-size: 12px;">
    Sample Transformations - All images are shifted, center-cropped, and then scaled to <code>3 x 64 x 64</code><sup>4</sup>
    </figcaption>
    </i>
    <figure>
    </center>
    <hr />
    <h3 id="aws-system-architecture">AWS System Architecture</h3>
    <center>
    <figure>
    <img alt="training_samples_eu" style="padding-top: 20px;" align="center" width="600" src="./images/infra/arch.png"> <i>
    <figcaption style="font-size: 12px;">
    Simplified Model Training Architecture
    </figcaption>
    </i>
    <figure>
    </center>
    <p>All infrastructure for this project is hosted on AWS. If you’d like a user-guide for deploying the architecture yourself, I’d direct you to my infrastructure <a href="https://github.com/DMW2151/msls-infra">repo</a>. All training resources run in a single VPC with two subnets (1 public, 1 private) in the same availability zone. I deployed the following instances to the VPC’s private subnet and accessed them via SSH through a jump-instance deployed to the public subnet.</p>
    <ul>
    <li><p><strong>training-prod</strong> — A model training instance, either <code>DL1</code> or a cost-comparable GPU instance (<code>P</code>-type).</p></li>
    <li><p><strong>training-nb</strong> — A small Sagemaker instance used for interactive model development, model evaluation, and generating plots.</p></li>
    <li><p><strong>metrics</strong> — A small instance used to host metrics containers. This machine ran:</p>
    <ul>
    <li><a href="https://www.tensorflow.org/tensorboard">Tensorboard</a> — A tool for visualizing <em>machine learning metrcs</em> during training.</li>
    <li><a href="https://grafana.com/">Grafana</a> — An analytics and monitoring tool. I configured Grafana to visualize <em>machine-level</em> metrics from our training instances.</li>
    </ul></li>
    </ul>
    <p>Each of these instances has access to an AWS Elastic Filesystem (EFS) with MSLS data. Using EFS saved me hours of data transfer in development and allowed me to pass model checkpoints between machines (i.e. between <em>training-prod</em> and <em>training-nb</em>). Of course, EFS is slower than NVME EBS volumes, but because the data (~40GB) fits comfortably in memory of our training devices this wasn’t a bottleneck in training speed after the initial load.</p>
    <hr />
    <h3 id="modifications-for-training-on-gaudi-accelerated-instances-dl1">Modifications for Training on Gaudi Accelerated Instances (<code>DL1</code>)</h3>
    <p>I started with a standard PyTorch model running on the GPU before instrumenting it with the code to run on the HPU. Migrating a model to run on HPUs require some changes, most of which are highlighted in the migration <a href="https://docs.habana.ai/en/latest/Migration_Guide/Migration_Guide.html#porting-simple-pyt-model">guide</a>. In general, a few changed imports allow the PyTorch Habana bridge to drive the execution of deep learning models on the Habana Gaudi device. Specifically, I made the following changes for the Gaudi accelerated instances:</p>
    <ul>
    <li><p>Swap out a standard <code>pytorch.DataLoader</code> for <code>habana_dataloader.HabanaDataLoader</code>. Under the right <a href="https://docs.habana.ai/en/v1.1.0/PyTorch_User_Guide/PyTorch_User_Guide.html#habana-data-loader">circumstances</a>, <code>HabanaDataLoader</code> can yield better performance that the native <code>DataLoader</code>. Even without acceleration, I can still use the <code>HabanaDataLoader</code> with a high <code>num_workers</code> parameter to quickly shuttle data onto the device.</p></li>
    <li><p>Use <code>Lazy Mode</code>. <a href="https://docs.habana.ai/en/v1.1.0/PyTorch_User_Guide/PyTorch_User_Guide.html#lazy-mode">Lazy Mode</a> provides the SynapseAI graph compiler the opportunity to optimize the device execution for multiple ops.</p></li>
    <li><p>Use <code>FusedAdamW</code> over <code>AdamW</code>. <code>FusedAdamW</code> can batch the element-wise updates applied to all the model’s parameters into one or a few kernel launches rather than a single kernel for each parameter. This is a custom optimizer for Habana devices and should yield some performance improvements over <code>AdamW</code>.</p></li>
    </ul>
    <hr />
    <h3 id="dcgan-results">DCGAN Results</h3>
    <p>TBD</p>
    <hr />
    <h3 id="comparative-performance">Comparative Performance</h3>
    <p>TBD</p>
    <hr />
    <h3 id="appendix-1---comparable-instance-selection">Appendix 1 - Comparable Instance Selection</h3>
    <p>Using <a href="https://instances.vantage.sh/">instances.vantage.sh</a> and <code>aws describe-instances</code>, I aggregated data for all EC2 instances available in <code>us-east-1</code> with between 2 and 8 GPUs. These machines range from those with GPUs that are designed for graphics workloads (e.g. <code>G3</code> instances with Tesla <code>M60</code>s) to top-of-the line training instances (e.g. <code>P4</code> instances with <code>A100</code>s). I relied exclusively on Nvidia’s most recent <a href="https://developer.nvidia.com/deep-learning-performance-training-inference">resnext-101 benchmarks</a> as a proxy for my model’s performance. On price, <code>p3.8xlarge</code> instances are the most similar to the <code>DL1</code> and offer 4 <code>V100</code>. Although <code>g4dn.12xlarge</code>(<code>T4</code>) and <code>p2.8xlarge</code> (<code>K80</code>) instances are priced well relative to their performance, I elected to only run a full test on the <code>p3.8xlarge</code>.</p>
    <table>
    <caption>Table A.1.1 - Possible Comparable GPU Instances</caption>
    <thead>
    <tr class="header">
    <th>API Name</th>
    <th>Memory (GiB)</th>
    <th>VCPUs</th>
    <th>GPUs</th>
    <th>GPU Model</th>
    <th>GPU Mem (GiB)</th>
    <th>USD/Hr</th>
    </tr>
    </thead>
    <tbody>
    <tr class="odd">
    <td>g3.8xlarge</td>
    <td>244</td>
    <td>32</td>
    <td>2</td>
    <td>NVIDIA Tesla M60</td>
    <td>16</td>
    <td>$2.28</td>
    </tr>
    <tr class="even">
    <td>g3.16xlarge</td>
    <td>488</td>
    <td>64</td>
    <td>4</td>
    <td>NVIDIA Tesla M60</td>
    <td>32</td>
    <td>$4.56</td>
    </tr>
    <tr class="odd">
    <td>p2.8xlarge</td>
    <td>488</td>
    <td>32</td>
    <td>8</td>
    <td>NVIDIA Tesla K80</td>
    <td>96</td>
    <td>$7.20</td>
    </tr>
    <tr class="even">
    <td>g4dn.12xlarge</td>
    <td>192</td>
    <td>48</td>
    <td>4</td>
    <td>NVIDIA T4 Tensor Core</td>
    <td>64</td>
    <td>$3.91</td>
    </tr>
    <tr class="odd">
    <td>g4dn.metal</td>
    <td>384</td>
    <td>96</td>
    <td>8</td>
    <td>NVIDIA T4 Tensor Core</td>
    <td>128</td>
    <td>$7.82</td>
    </tr>
    <tr class="even">
    <td>g5.12xlarge</td>
    <td>192</td>
    <td>48</td>
    <td>4</td>
    <td>NVIDIA A10G</td>
    <td>96</td>
    <td>$5.67</td>
    </tr>
    <tr class="odd">
    <td>g5.24xlarge</td>
    <td>384</td>
    <td>96</td>
    <td>4</td>
    <td>NVIDIA A10G</td>
    <td>96</td>
    <td>$8.14</td>
    </tr>
    <tr class="even">
    <td>g5.48xlarge</td>
    <td>768</td>
    <td>192</td>
    <td>8</td>
    <td>NVIDIA A10G</td>
    <td>192</td>
    <td>$16.29</td>
    </tr>
    <tr class="odd">
    <td>p3.8xlarge</td>
    <td>244</td>
    <td>32</td>
    <td>4</td>
    <td>NVIDIA Tesla V100</td>
    <td>64</td>
    <td>$12.24</td>
    </tr>
    <tr class="even">
    <td>p3.16xlarge</td>
    <td>488</td>
    <td>64</td>
    <td>8</td>
    <td>NVIDIA Tesla V100</td>
    <td>128</td>
    <td>$24.48</td>
    </tr>
    <tr class="odd">
    <td>p3dn.24xlarge</td>
    <td>768</td>
    <td>96</td>
    <td>8</td>
    <td>NVIDIA Tesla V100</td>
    <td>256</td>
    <td>$31.21</td>
    </tr>
    <tr class="even">
    <td>p4d.24xlarge</td>
    <td>1152</td>
    <td>96</td>
    <td>8</td>
    <td>NVIDIA A100</td>
    <td>320</td>
    <td>$32.77</td>
    </tr>
    </tbody>
    </table>
    <h3 id="citations">Citations</h3>
    <p><strong><sup>1</sup></strong> <em>“Generative Adversarial Networks.” Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. ArXiv 2014.</em></p>
    <p><strong><sup>2</sup></strong> <em>Radford, Alec, Luke Metz, and Soumith Chintala. “Unsupervised representation learning with deep convolutional generative adversarial networks.” arXiv preprint arXiv:1511.06434 (2015).</em></p>
    <p><strong><sup>3</sup></strong> <em>F. Warburg, S. Hauberg, M. Lopez-Antequera, P. Gargallo, Y. Kuang, and J. Civera. Mapillary Street-Level Sequences: A Dataset for Lifelong Place Recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020</em></p>
    <p><strong><sup>4</sup></strong> <em>File:NYC 14th Street looking west 12 2005.jpg. (2020, September 13). Wikimedia Commons, the free media repository. Retrieved 23:09, January 25, 2022 from https://commons.wikimedia.org/w/index.php?title=File:NYC_14th_Street_looking_west_12_2005.jpg&amp;oldid=457344851</em></p>
  </body>
</html>
