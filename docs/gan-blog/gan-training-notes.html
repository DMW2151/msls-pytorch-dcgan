<!doctype html>
<html lang="en">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon"> 
    <meta charset="utf-8">
    <meta name="author" content='Dustin Wilson'>
    <meta name="date" content='2022-01-29'>
    <title>Model Training Notes</title>
    <style>
            html {
              line-height: 1.5;
              font-family: Georgia, serif;
              font-size: 16px;
              color: #1a1a1a;
              background-color: #202025;
            }
            .header {
              text-align: center;
              background: gray;
              color: white;
            }
            body {
              margin: 0 auto;
              max-width: 52em;
              padding-left: 50px;
              padding-right: 50px;
              padding-top: 50px;
              padding-bottom: 50px;
              hyphens: auto;
              overflow-wrap: break-word;
              text-rendering: optimizeLegibility;
              font-kerning: normal;
            }
            @media (max-width: 600px) {
              body {
                font-size: 0.9em;
                padding: 1em;
              }
            }
            @media print {
              body {
                background-color: transparent;
                color: black;
                font-size: 12pt;
              }
              p, h2, h3 {
                orphans: 3;
                widows: 3;
              }
              h2, h3, h4 {
                page-break-after: avoid;
              }
            }
            p {
              margin: 1em 0;
            }
            a {
              color: #5858d9;
            }
            a:visited {
              color: #9292f2;
            }
            img {
              max-width: 100%;
            }
            h1, h2, h3, h4, h5, h6 {
              margin-top: 1.4em;
            }
            h5, h6 {
              font-size: 1em;
              font-style: italic;
            }
            h6 {
              font-weight: normal;
            }
            ol, ul {
              padding-left: 1.7em;
              margin-top: 1em;
            }
            li > ol, li > ul {
              margin-top: 0;
            }
            blockquote {
              margin: 1em 0 1em 1.7em;
              padding-left: 1em;
              border-left: 2px solid #e6e6e6;
            }
            mark {
              background: lightgoldenrodyellow
            }
            code {
              font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
              font-size: 85%;
              margin: 0;
              color: #FFFBC8;
            }
            pre {
              margin: 1em 0;
              overflow: auto;
            }
            pre code {
              padding: 0;
              overflow: visible;
              overflow-wrap: normal;
              color: #ebebe6;
            }
            .sourceCode {
             background-color: #202025;
             overflow: visible;
            }
            hr {
              background-color: #ebebe6;
              border: none;
              height: 2px;
              margin: 1em 0;
            }
            .gan-test-img {
              transition: background-color .2s;
            }
            .gan-test-img:hover {
              mix-blend-mode: hard-light;
              width: min-content;
            }
            table {
              margin: 1em 0;
              border-collapse: collapse;
              width: 100%;
              overflow-x: auto;
              font-variant-numeric: lining-nums tabular-nums;
              color: #ebebe6;
              margin-left: auto;
              margin-right: auto;
            }
            table caption {
              margin-bottom: 0.75em;
            }
            tbody {
              margin-top: 0.5em;
              border-top: 1px solid #ebebe6;
              border-bottom: 1px solid #ebebe6;
              margin-left: auto;
              margin-right: auto;
            }
            th {
              border-top: 1px solid #202025;
              padding: 0.25em 0.5em 0.25em 0.5em;
            }
            td {
              padding: 0.125em 0.5em 0.25em 0.5em;
              color: #ebebe6;
            }
            header {
              margin-bottom: 4em;
              text-align: center;
            }
            #TOC li {
              list-style: none;
            }
            #TOC a:not(:hover) {
              text-decoration: none;
            }
            code{white-space: pre-wrap;}
            span.smallcaps{font-variant: small-caps;}
            span.underline{text-decoration: underline;}
            div.column{display: inline-block; vertical-align: top; width: 50%;}
            div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
            ul.task-list{list-style: none;}
            pre > code.sourceCode { white-space: pre; position: relative; }
            pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
            pre > code.sourceCode > span:empty { height: 1.2em; }
            .sourceCode { overflow: visible; }
            code.sourceCode > span { color: inherit; text-decoration: inherit; }
            div.sourceCode { margin: 1em 0; }
            pre.sourceCode { margin: 0; }
            @media screen {
            div.sourceCode { overflow: auto; }
            }
            @media print {
            pre > code.sourceCode { white-space: pre-wrap; }
            pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
            }
            pre.numberSource code
              { counter-reset: source-line 0; }
            pre.numberSource code > span
              { position: relative; left: -4em; counter-increment: source-line; }
            pre.numberSource code > span > a:first-child::before
              { content: counter(source-line);
                position: relative; left: -1em; text-align: right; vertical-align: baseline;
                border: none; display: inline-block;
                -webkit-touch-callout: none; -webkit-user-select: none;
                -khtml-user-select: none; -moz-user-select: none;
                -ms-user-select: none; user-select: none;
                padding: 0 4px; width: 4em;
                color: #FFFBC8;
              }
            pre.numberSource { margin-left: 3em; border-left: 1px solid #FFFBC8;  padding-left: 4px; }
            div.sourceCode
              {   }
            @media screen {
            pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
            }
            code span.al { color: #FFFBC8; font-weight: bold; } /* Alert */
            code span.an { color: #FFFBC8; font-weight: bold; font-style: italic; } /* Annotation */
            code span.at { color: #FFFBC8; } /* Attribute */
            code span.bn { color: #FFFBC8; } /* BaseN */
            code span.bu { } /* BuiltIn */
            code span.cf { color: #FFFBC8; font-weight: bold; } /* ControlFlow */
            code span.ch { color: #FFFBC8; } /* Char */
            code span.cn { color: #FFFBC8; } /* Constant */
            code span.co { color: #FFFBC8; font-style: italic; } /* Comment */
            code span.cv { color: #FFFBC8; font-weight: bold; font-style: italic; } /* CommentVar */
            code span.do { color: #FFFBC8; font-style: italic; } /* Documentation */
            code span.dt { color: #FFFBC8; } /* DataType */
            code span.dv { color: #FFFBC8; } /* DecVal */
            code span.er { color: #FFFBC8; font-weight: bold; } /* Error */
            code span.ex { } /* Extension */
            code span.fl { color: #FFFBC8; } /* Float */
            code span.fu { color: #FFFBC8; } /* Function */
            code span.im { } /* Import */
            code span.in { color: #FFFBC8; font-weight: bold; font-style: italic; } /* Information */
            code span.kw { color: #FFFBC8; font-weight: bold; } /* Keyword */
            code span.op { color: #FFFBC8; } /* Operator */
            code span.ot { color: #FFFBC8; } /* Other */
            code span.pp { color: #FFFBC8; } /* Preprocessor */
            code span.sc { color: #FFFBC8; } /* SpecialChar */
            code span.ss { color: #FFFBC8; } /* SpecialString */
            code span.st { color: #FFFBC8; } /* String */
            code span.va { color: #FFFBC8; } /* Variable */
            code span.vs { color: #FFFBC8; } /* VerbatimString */
            code span.wa { color: #FFFBC8; font-weight: bold; font-style: italic; } /* Warning */
            .display.math{display: block; text-align: center; margin: 0.5rem auto;}
          </style>
  </head>
  <body style="background-color:#202025; color: #e6e6e6"></body>
    <header>
        <div style="background-color:#202025; color: #e6e6e6" class="header">
            <a href="./trained-a-gan.html" style="padding: 10px; font-size: 12px;">Project Overview</a>
            <a href="./gan-training-notes.html" style="padding: 10px; font-size: 12px;">Infrastructure & Model Guide</a>
            <a href="./gallery.html" style="padding: 10px; font-size: 12px;">Gallery</a>
        </div>
    </header>
    <h1> Model Training Notes </h1>
    <p>Dustin Wilson &#8212 January 29, 2022</p>
    <p>This post is included along with my <a href="./trained-a-gan.html">main post</a> to serve as a user guide for those interested in training their own GANs with the code presented <a href="https://github.com/DMW2151/msls-pytorch-dcgan">here</a>.</p>
    <hr />
    <h2 id="infrastructure">Infrastructure</h2>
    <h3 id="the-full-infrastructure">The Full Infrastructure</h3>
    <p><strong>NOTICE:</strong> <mark> This module deploys live resources into your AWS account, please be mindful of the costs associated with those resources. This module does not download (nor provision pipelines to download) MSLS data, you may access it from <a href="https://www.mapillary.com/datasets">Mapillary</a>.</mark></p>
    <p>All infrastructure for the project is available <a href="https://github.com/DMW2151/msls-dcgan-infra">here</a> and should be readily accessible if you’re familiar with Terraform. If you’re not comfortable with Terraform, I would recommend reading the following resources to make sure you’re <strong>safely</strong> deploying resources to your account.</p>
    <ul>
    <li><a href="https://learn.hashicorp.com/collections/terraform/aws-get-started">Terraform on AWS</a></li>
    </ul>
    <p>If you are familiar with Terraform, I’d ask that you still be quite cautious, as this module deploys resources that can easily top a few hundred dollars / day if left unattended (mostly the <code>DL1</code> instance, but SageMaker and EBS can add up, too). Keep the following in mind:</p>
    <ul>
    <li>Remember to change the state-file’s S3-backend to a bucket you own!</li>
    <li>Remember to change regions to one with <code>DL1</code> instances offered!</li>
    </ul>
    <h3 id="model-training-only">Model Training Only</h3>
    <p>If you’re just interested in training a GAN (and not the intermediate training or hardware metrics), I would recommend provisioning your own infrastructure, cloning the model <a href="https://github.com/DMW2151/msls-pytorch-dcgan">repo</a> onto a suitable EC2 instance, and pip installing the training package with <code>pip -e install /path/to/msls/package/model/</code>.</p>
    <h3 id="note-on-disk-performance">Note on Disk Performance</h3>
    <p>Because of the image size I’m using in the network, this problem is a bit inbalanced. On a high-performance GPU or the HPU, disk becomes a bottleneck rather quickly. The relative size of the training images (<code>3 x 64 x 64</code>) vs. the size of the images pulled off disk (~<code>3 x 480 x 480</code>) leads to constant stress on the disk. In this case, adding extra resources to train the model has a sharply diminished effect.</p>
    <p>My Terraform provisions a GP3 volume w. 8000 IOPs to (partially) handle for this, but if you’re using a modern GPU or HPU, I would suggest training on the best disk available to your instance. On the <code>DL1</code>, I saw a meaningful performance improvement from using ephemeral NVME storage over the GP3 volume.</p>
    <hr />
    <h2 id="data-preparation">Data Preparation</h2>
    <p>The MSLS data is available for download <a href="https://www.mapillary.com/dataset/places">here</a>. Please note that since at least December they’ve had <a href="https://github.com/mapillary/mapillary_sls/issues/23">sporadic performance</a> on their download site. The data is available in batches, and at the very least Batch 6 (1.6 GB compressed) should download without issue.</p>
    <p>The data should not be accessed programmatically, once you agree to the Mapillary TOS you’ll recieve a signed URL for each batch. I used the following to download and unzip the data.</p>
    <div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>    <span class="co">#! /bin/bash</span></span>
    <span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-k</span> <span class="at">-XGET</span> <span class="va">${A_SIGNED_DOWNLOAD_URL}</span> <span class="at">--output</span> /data/msls_<span class="va">${BATCH_NUM}</span>.zip <span class="kw">&amp;&amp;</span><span class="dt">\</span></span>
    <span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sudo</span> unzip <span class="at">-qq</span> /data/msls_<span class="va">${BATCH_NUM}</span>.zip <span class="at">-d</span> /data/imgs</span></code></pre></div>
    <hr />
    <h2 id="model-training">Model Training</h2>
    <p>I designed model training to be as simple as possible. On a GPU instance, the following should be adequate to begin training. All flags for <code>python3 -m msls.run_dcgan</code> are shown below. Please refer to the full documentation to understand what each flag and value does.</p>
    <div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>    <span class="co">#! /bin/bash</span></span>
    <span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
    <span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> pip install <span class="at">--upgrade</span> pip <span class="kw">&amp;&amp;</span><span class="dt">\</span></span>
    <span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sudo</span> <span class="at">-H</span> pip3 install /path/to/msls/package/model</span>
    <span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
    <span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> msls.run_dcgan <span class="dt">\</span></span>
    <span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">-c</span> <span class="st">&#39;{&quot;name&quot;: &quot;msls-dcgan-128&quot;, &quot;root&quot;: &quot;/efs/trained_model/&quot;, &quot;log_frequency&quot;: 50, &quot;save_frequency&quot;: 1,}&#39;</span> <span class="dt">\</span></span>
    <span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">-t</span> <span class="st">&#39;{&quot;nc&quot;: 3, &quot;nz&quot;: 256, &quot;ngf&quot;: 256, &quot;ndf&quot;: 64, &quot;lr&quot;: 0.0002, &quot;beta1&quot;: 0.5, &quot;beta2&quot;: 0.999, &quot;batch_size&quot;: 256, &quot;img_size&quot;: 128, &quot;weight_decay&quot;: 0.05}&#39;</span><span class="dt">\</span></span>
    <span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">--s_epoch</span> 0 <span class="dt">\</span></span>
    <span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">--n_epoch</span> 16 <span class="dt">\</span></span>
    <span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">--dataroot</span> /data/imgs/train_val/helsinki <span class="dt">\</span></span>
    <span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">--logging</span> True <span class="dt">\</span></span>
    <span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">--profile</span> True  <span class="dt">\</span></span>
    <span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">--s3_bucket</span> <span class="st">&#39;dmw2151-habana-model-outputs&#39;</span></span></code></pre></div>
    <p>In the case above, the <code>msls</code> package will fall back to training on the GPU because no HPU is available. When a call to <code>load_habana_module()</code> succeeds, the model will prefer to train on the HPU. Once in a container (or on host) with the Habana modules properly installed, the run same commands as above.</p>
    <div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>    <span class="co">#! /bin/bash</span></span>
    <span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> run <span class="at">-ti</span> <span class="at">--runtime</span><span class="op">=</span>habana <span class="dt">\</span></span>
    <span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">-e</span> HABANA_VISIBLE_DEVICES=all <span class="dt">\</span></span>
    <span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">-e</span> OMPI_MCA_btl_vader_single_copy_mechanism=none <span class="dt">\</span></span>
    <span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">--cap-add</span><span class="op">=</span>sys_nice <span class="dt">\</span></span>
    <span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">--net</span><span class="op">=</span>host <span class="dt">\</span></span>
    <span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">-v</span> /data:/data <span class="dt">\</span></span>
    <span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">-v</span> /efs:/efs <span class="dt">\</span></span>
    <span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">-v</span> /root/msls-pytorch-dcgan:/root/msls-pytorch-dcgan/ <span class="dt">\</span></span>
    <span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">--ipc</span><span class="op">=</span>host <span class="dt">\</span></span>
    <span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    vault.habana.ai/gaudi-docker/1.2.0/ubuntu18.04/habanalabs/pytorch-installer-1.10.0:1.2.0-585</span></code></pre></div>
    <p>When using a container on a DLAMI with Habana drivers, remember to mount data and metadata folders with Docker’s <code>-v</code> flag. Finally, If you want a truly guided experience, you can use the notebooks in <code>./model/notebooks</code> to train in a SageMaker notebook (as of writing, <code>DL1</code> is not a supported SageMaker instance type).</p>
    <footer class="site-footer">
        <span class="site-footer-owner"> Maintained by <a href="https://github.com/DMW2151">DMW2151</a>.</span>
    </footer>
  </body>
</html>
