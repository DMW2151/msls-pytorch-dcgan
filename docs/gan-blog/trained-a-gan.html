<!doctype html>
<html lang="en">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon"> 
    <meta charset="utf-8">
    <meta name="author" content='Dustin Wilson'>
    <meta name="date" content='2022-01-29'>
    <title>Generative Street-Level Imagery on DL1 Instances</title>
    <style>
            html {
              line-height: 1.5;
              font-family: Georgia, serif;
              font-size: 16px;
              color: #1a1a1a;
              background-color: #202025;
            }
            .header {
              text-align: center;
              background: gray;
              color: white;
            }
            body {
              margin: 0 auto;
              max-width: 52em;
              padding-left: 50px;
              padding-right: 50px;
              padding-top: 50px;
              padding-bottom: 50px;
              hyphens: auto;
              overflow-wrap: break-word;
              text-rendering: optimizeLegibility;
              font-kerning: normal;
            }
            @media (max-width: 600px) {
              body {
                font-size: 0.9em;
                padding: 1em;
              }
            }
            @media print {
              body {
                background-color: transparent;
                color: black;
                font-size: 12pt;
              }
              p, h2, h3 {
                orphans: 3;
                widows: 3;
              }
              h2, h3, h4 {
                page-break-after: avoid;
              }
            }
            p {
              margin: 1em 0;
            }
            a {
              color: #5858d9;
            }
            a:visited {
              color: #9292f2;
            }
            img {
              max-width: 100%;
            }
            h1, h2, h3, h4, h5, h6 {
              margin-top: 1.4em;
            }
            h5, h6 {
              font-size: 1em;
              font-style: italic;
            }
            h6 {
              font-weight: normal;
            }
            ol, ul {
              padding-left: 1.7em;
              margin-top: 1em;
            }
            li > ol, li > ul {
              margin-top: 0;
            }
            blockquote {
              margin: 1em 0 1em 1.7em;
              padding-left: 1em;
              border-left: 2px solid #e6e6e6;
            }
            code {
              font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
              font-size: 85%;
              margin: 0;
              color: #FFFBC8;
            }
            pre {
              margin: 1em 0;
              overflow: auto;
            }
            pre code {
              padding: 0;
              overflow: visible;
              overflow-wrap: normal;
              color: #ebebe6;
            }
            .sourceCode {
             background-color: #202025;
             overflow: visible;
            }
            hr {
              background-color: #ebebe6;
              border: none;
              height: 2px;
              margin: 1em 0;
            }
            .gan-test-img {
              transition: background-color .2s;
            }
            .gan-test-img:hover {
              mix-blend-mode: hard-light;
              width: min-content;
            }
            table {
              margin: 1em 0;
              border-collapse: collapse;
              width: 100%;
              overflow-x: auto;
              font-variant-numeric: lining-nums tabular-nums;
              color: #ebebe6;
              margin-left: auto;
              margin-right: auto;
            }
            table caption {
              margin-bottom: 0.75em;
            }
            tbody {
              margin-top: 0.5em;
              border-top: 1px solid #ebebe6;
              border-bottom: 1px solid #ebebe6;
              margin-left: auto;
              margin-right: auto;
            }
            th {
              border-top: 1px solid #202025;
              padding: 0.25em 0.5em 0.25em 0.5em;
            }
            td {
              padding: 0.125em 0.5em 0.25em 0.5em;
              color: #ebebe6;
            }
            header {
              margin-bottom: 4em;
              text-align: center;
            }
            #TOC li {
              list-style: none;
            }
            #TOC a:not(:hover) {
              text-decoration: none;
            }
            code{white-space: pre-wrap;}
            span.smallcaps{font-variant: small-caps;}
            span.underline{text-decoration: underline;}
            div.column{display: inline-block; vertical-align: top; width: 50%;}
            div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
            ul.task-list{list-style: none;}
            pre > code.sourceCode { white-space: pre; position: relative; }
            pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
            pre > code.sourceCode > span:empty { height: 1.2em; }
            .sourceCode { overflow: visible; }
            code.sourceCode > span { color: inherit; text-decoration: inherit; }
            div.sourceCode { margin: 1em 0; }
            pre.sourceCode { margin: 0; }
            @media screen {
            div.sourceCode { overflow: auto; }
            }
            @media print {
            pre > code.sourceCode { white-space: pre-wrap; }
            pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
            }
            pre.numberSource code
              { counter-reset: source-line 0; }
            pre.numberSource code > span
              { position: relative; left: -4em; counter-increment: source-line; }
            pre.numberSource code > span > a:first-child::before
              { content: counter(source-line);
                position: relative; left: -1em; text-align: right; vertical-align: baseline;
                border: none; display: inline-block;
                -webkit-touch-callout: none; -webkit-user-select: none;
                -khtml-user-select: none; -moz-user-select: none;
                -ms-user-select: none; user-select: none;
                padding: 0 4px; width: 4em;
                color: #FFFBC8;
              }
            pre.numberSource { margin-left: 3em; border-left: 1px solid #FFFBC8;  padding-left: 4px; }
            div.sourceCode
              {   }
            @media screen {
            pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
            }
            code span.al { color: #FFFBC8; font-weight: bold; } /* Alert */
            code span.an { color: #FFFBC8; font-weight: bold; font-style: italic; } /* Annotation */
            code span.at { color: #FFFBC8; } /* Attribute */
            code span.bn { color: #FFFBC8; } /* BaseN */
            code span.bu { } /* BuiltIn */
            code span.cf { color: #FFFBC8; font-weight: bold; } /* ControlFlow */
            code span.ch { color: #FFFBC8; } /* Char */
            code span.cn { color: #FFFBC8; } /* Constant */
            code span.co { color: #FFFBC8; font-style: italic; } /* Comment */
            code span.cv { color: #FFFBC8; font-weight: bold; font-style: italic; } /* CommentVar */
            code span.do { color: #FFFBC8; font-style: italic; } /* Documentation */
            code span.dt { color: #FFFBC8; } /* DataType */
            code span.dv { color: #FFFBC8; } /* DecVal */
            code span.er { color: #FFFBC8; font-weight: bold; } /* Error */
            code span.ex { } /* Extension */
            code span.fl { color: #FFFBC8; } /* Float */
            code span.fu { color: #FFFBC8; } /* Function */
            code span.im { } /* Import */
            code span.in { color: #FFFBC8; font-weight: bold; font-style: italic; } /* Information */
            code span.kw { color: #FFFBC8; font-weight: bold; } /* Keyword */
            code span.op { color: #FFFBC8; } /* Operator */
            code span.ot { color: #FFFBC8; } /* Other */
            code span.pp { color: #FFFBC8; } /* Preprocessor */
            code span.sc { color: #FFFBC8; } /* SpecialChar */
            code span.ss { color: #FFFBC8; } /* SpecialString */
            code span.st { color: #FFFBC8; } /* String */
            code span.va { color: #FFFBC8; } /* Variable */
            code span.vs { color: #FFFBC8; } /* VerbatimString */
            code span.wa { color: #FFFBC8; font-weight: bold; font-style: italic; } /* Warning */
            .display.math{display: block; text-align: center; margin: 0.5rem auto;}
          </style>
  </head>
  <body style="background-color:#202025; color: #e6e6e6"></body>
    <header>
        <div style="background-color:#202025; color: #e6e6e6" class="header">
            <a href="./trained-a-gan.html" style="padding: 10px; font-size: 12px;">Project Overview</a>
            <a href="./gan-training-notes.html" style="padding: 10px; font-size: 12px;">Infrastructure & Model Guide</a>
            <a href="./gallery.html" style="padding: 10px; font-size: 12px;">Gallery</a>
        </div>
    </header>
    <h1> Generative Street-Level Imagery on DL1 Instances </h1>
    <p>Dustin Wilson &#8212 January 29, 2022</p>
    <hr />
    <center>
    <figure class="image">
    <img src="./images/gan/001.gif" height="auto" width="188" style="padding: 20px; border-radius: 2px"> <img src="./images/gan/002.gif" height="auto" width="188" style="padding: 20px; border-radius: 2px"> <img src="./images/gan/003.gif" height="auto" width="188" style="padding: 20px; border-radius: 2px"> <i>
    <figcaption style="font-size: 12px;">
    Nowhere, USA - Experimental Output - Scenes created by interpolating between sequences of generated frames
    </figcaption>
    </i>
    </figure>
    </center>
    <p>For a few months now, I’ve wanted to create something like <a href="https://thispersondoesnotexist.com/">ThisPersonDoesNotExist</a> for street scenes. Luckily, the <a href="https://amazon-ec2-dl1.devpost.com">AWS Deep Learning Challenge</a> gave me an excuse to do so. At a high level, my project involved re-implementing elements of two foundational papers in generative computer vision and then training that model on over 1.1 million street-level images.</p>
    <p>It’s not a novel idea, but enough work has been done in this field that I was able to read up on the literature, implement generative models, and reason about architectural and performance tradeoffs. The challenge encouraged participants to use AWS’ <code>DL1</code> instances to scale deep learning model training on HPUs. With that in mind, I instrumented my code to train on both GPU and Gaudi accelerators, and then performed a comparative analysis of performance across training environments.</p>
    <ul>
    <li><a href="#Theory-and-Background">Theory and Background</a></li>
    <li><a href="#Mapillary-Street-Level-Imagery-Data">Mapillary Street Level Imagery Data</a></li>
    <li><a href="#DCGAN-Results">DCGAN Results</a>
    <ul>
    <li><a href="#Results">Results</a></li>
    <li><a href="#Modeling-Considerations">Modeling Considerations</a></li>
    </ul></li>
    <li><a href="#AWS-System-Architecture">AWS System Architecture</a></li>
    <li><a href="#Evaluating-a-First-Training-run-on-GPU-Instances">Evaluating a First Training run on GPU Instances</a></li>
    <li><a href="#Modifications-for-Training-on-Gaudi-Accelerated-Instances">Modifications for Training on Gaudi Accelerated Instances</a></li>
    <li><a href="#Comparative-Performance">Comparative Performance</a></li>
    <li><a href="#Appendix-1---Modeling-Choices">Appendix 1 - Modeling Choices</a></li>
    <li><a href="#Appendix-2---Comparable-Instance-Selection">Appendix 2 - Comparable Instance Selection</a></li>
    <li><a href="#Appendix-3---PIL-Benchmarks">Appendix 3 - PIL Benchmarks</a></li>
    <li><a href="#Appendix-4---Assessing-GAN-performance">Appendix 4 - Assessing GAN performance</a></li>
    <li><a href="#Citations">Citations</a></li>
    </ul>
    <hr />
    <h3 id="theory-and-background">Theory and Background</h3>
    <p>In this project I re-implement elements of Ian Goodfellow’s <a href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">Generative Adversarial Networks (2014)</a><sup>1</sup> and Alec Radford’s <a href="https://arxiv.org/pdf/1511.06434.pdf">Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks (2016)</a><sup>2</sup> papers in PyTorch. Both papers are concerned with the development of GANs, Generative Adversarial Networks.</p>
    <p>Before discussing specific elements of the project, let’s discuss the <em>way</em> GANs work. Put simply, GANs consist of two competing functions. A generator (<code>G</code>) tries to create believable data and a discriminator (<code>D</code>) tries to maximize the probability it correctly classifies real and generated data.</p>
    <p><strong>Assume the following variables:</strong></p>
    <ul>
    <li><p><code>X</code> — Input data, in our case, an image with size <code>(3 x 64 x 64)</code></p></li>
    <li><p><code>D(X)</code> or <code>D</code> — Discriminator network which outputs the probability that an input, <code>X</code>, is real.</p></li>
    <li><p><code>G(Z)</code> or <code>G</code> — Generator network that deterministically creates data in the shape of <code>X</code>. In practice, an image with size <code>(3 x 64 x 64)</code>.</p></li>
    <li><p><code>Z</code> — Random noise to seed the generator. In practice, a <code>(1 x 100)</code> vector drawn from a standard normal distribution.</p></li>
    <li><p><code>D(G(Z))</code> — Given an output of the generator, the probability that the discriminator believes the image to be real. A high <code>D(G(Z))</code> suggests the generator has “tricked” the discriminator.</p></li>
    </ul>
    <p>The critical steps in each training iteration involve measuring the values of the following terms. For the formula-inclined, the GAN is simply maximizing the following function:</p>
    <center>
    <code>min​</code><sub><code>G</code></sub><code>max​</code><sub><code>V</code></sub><code>(D,G) = E</code><sub><code>x∼pdata​(x)</code></sub>​<code>[logD(x)] + E</code><sub><code>z∼pz​(z)​</code></sub><code>[log(1−D(G(z)))]</code>
    </center>
    <ul>
    <li><p><code>E</code><sub><code>x∼pdata​(x)</code></sub>​<code>[logD(x)]</code> — The expected value of <code>D</code>’s predictions when given samples from the real batch. Remember, <code>D(x)</code> produces a probability, thus a perfect discriminator would return values near <em>0</em>.</p></li>
    <li><p><code>E</code><sub><code>z∼pz​(z)​</code></sub><code>[log(1−D(G(z)))]</code> — The expected value of <code>D</code>’s prediction when given samples produced from <code>G(Z)</code>, Because all images in this batch are fake, a better discriminator would predict a lower <code>D(G(Z))</code>, also returning values near <em>0</em>.</p></li>
    </ul>
    <p>In the DCGAN paper, the method by which this function is maximized is by putting batches of images through <code>D</code> and <code>G</code>, where both are convolutional neural networks with a specific layer structure.</p>
    <center>
    <figure>
    <img style="padding-top: 20px;" align="center" width="600" src="./images/translation/gan.png"> <i>
    <figcaption style="font-size: 12px;">
    DBGAN Generator Architecture - As diagramed by Radford, et. al <sup>4<sup>
    </figcaption>
    </i>
    <figure>
    </center>
    <hr />
    <h3 id="mapillary-street-level-imagery-data">Mapillary Street Level Imagery Data</h3>
    <center>
    <figure>
    <img alt="training_samples_eu" style="padding-top: 20px;" align="center" width="600" src="./images/translation/train_samples_eu.png"> <i>
    <figcaption style="font-size: 12px;">
    Training Samples From MSLS - Cropped and Transformed
    </figcaption>
    </i>
    <figure>
    </center>
    <p>Throughout this project, I used Mapillary’s Street-Level Sequences data (MSLS). Mapillary provides a platform for crowd-sourced maps and street-level imagery, and publishes computer vision research using data collected from this platform. Mapillary has made this and other data publicly available for <a href="https://www.mapillary.com/dataset/places">download</a> (<strong>Note</strong>: <a href="https://github.com/mapillary/mapillary_sls/issues/23">GH Issue</a>). In total, MSLS contains 1.6 million images from 30 major cities on six-continents and covers different seasons, weather, daylight conditions, structural settings, etc. The models discussed in this post here was trained on a sample of ~1.2M images with geographic distribution shown below. The remaining images were reserved for hyperparameter tuning, cross-validation, model evaluation, etc.</p>
    <table>
    <caption>Training Sample By Metro Area</caption>
    <thead>
    <tr class="header">
    <th style="text-align: left;">Metro Area</th>
    <th style="text-align: center;">% of Sample</th>
    <th style="text-align: right;">Approx. Count</th>
    </tr>
    </thead>
    <tbody>
    <tr class="odd">
    <td style="text-align: left;">Amman</td>
    <td style="text-align: center;">0.14%</td>
    <td style="text-align: right;">1,702</td>
    </tr>
    <tr class="even">
    <td style="text-align: left;">Amsterdam</td>
    <td style="text-align: center;">1.37%</td>
    <td style="text-align: right;">16,487</td>
    </tr>
    <tr class="odd">
    <td style="text-align: left;">Austin</td>
    <td style="text-align: center;">1.90%</td>
    <td style="text-align: right;">22,847</td>
    </tr>
    <tr class="even">
    <td style="text-align: left;">Bangkok</td>
    <td style="text-align: center;">3.26%</td>
    <td style="text-align: right;">39,055</td>
    </tr>
    <tr class="odd">
    <td style="text-align: left;">Boston</td>
    <td style="text-align: center;">1.27%</td>
    <td style="text-align: right;">15,204</td>
    </tr>
    <tr class="even">
    <td style="text-align: left;">Budapest</td>
    <td style="text-align: center;">17.67%</td>
    <td style="text-align: right;">212,015</td>
    </tr>
    <tr class="odd">
    <td style="text-align: left;">Goa</td>
    <td style="text-align: center;">1.11%</td>
    <td style="text-align: right;">13,307</td>
    </tr>
    <tr class="even">
    <td style="text-align: left;">Helsinki</td>
    <td style="text-align: center;">1.75%</td>
    <td style="text-align: right;">20,978</td>
    </tr>
    <tr class="odd">
    <td style="text-align: left;">London</td>
    <td style="text-align: center;">0.65%</td>
    <td style="text-align: right;">7,755</td>
    </tr>
    <tr class="even">
    <td style="text-align: left;">Manila</td>
    <td style="text-align: center;">0.53%</td>
    <td style="text-align: right;">6,416</td>
    </tr>
    <tr class="odd">
    <td style="text-align: left;">Melbourne</td>
    <td style="text-align: center;">15.58%</td>
    <td style="text-align: right;">186,908</td>
    </tr>
    <tr class="even">
    <td style="text-align: left;">Moscow</td>
    <td style="text-align: center;">18.14%</td>
    <td style="text-align: right;">217,594</td>
    </tr>
    <tr class="odd">
    <td style="text-align: left;">Nairobi</td>
    <td style="text-align: center;">0.06%</td>
    <td style="text-align: right;">725</td>
    </tr>
    <tr class="even">
    <td style="text-align: left;">Ottawa</td>
    <td style="text-align: center;">12.09%</td>
    <td style="text-align: right;">145,063</td>
    </tr>
    <tr class="odd">
    <td style="text-align: left;">Paris</td>
    <td style="text-align: center;">1.62%</td>
    <td style="text-align: right;">19,416</td>
    </tr>
    <tr class="even">
    <td style="text-align: left;">Phoenix</td>
    <td style="text-align: center;">12.56%</td>
    <td style="text-align: right;">150,642</td>
    </tr>
    <tr class="odd">
    <td style="text-align: left;">Sao Paulo</td>
    <td style="text-align: center;">4.65%</td>
    <td style="text-align: right;">55,793</td>
    </tr>
    <tr class="even">
    <td style="text-align: left;">San Fransisco</td>
    <td style="text-align: center;">0.43%</td>
    <td style="text-align: right;">5,133</td>
    </tr>
    <tr class="odd">
    <td style="text-align: left;">Tokyo</td>
    <td style="text-align: center;">3.49%</td>
    <td style="text-align: right;">41,845</td>
    </tr>
    <tr class="even">
    <td style="text-align: left;">Toronto</td>
    <td style="text-align: center;">1.27%</td>
    <td style="text-align: right;">15,176</td>
    </tr>
    <tr class="odd">
    <td style="text-align: left;">Trondheim</td>
    <td style="text-align: center;">1.07%</td>
    <td style="text-align: right;">12,888</td>
    </tr>
    <tr class="even">
    <td style="text-align: left;">Zurich</td>
    <td style="text-align: center;">0.51%</td>
    <td style="text-align: right;">6,081</td>
    </tr>
    <tr class="odd">
    <td style="text-align: left;"><strong>Total</strong></td>
    <td style="text-align: center;"></td>
    <td style="text-align: right;"><strong>1,199,556</strong></td>
    </tr>
    </tbody>
    </table>
    <p>Because the authors who developed MSLS for their <a href="https://research.mapillary.com/publication/cvpr20c">research</a><sup>3</sup> were specifically interested in place-recognition, the data is organized such that images of the same physical location appear multiple times under different conditions. The images from these sequences are very highly correlated and reduce the diversity of the training set far more than a single repeated image.</p>
    <p>The effect of multi-image sequences was reduced by applying random transformations on each image. MSLS contains images up to <code>(3 x 640 x 480)</code>. Because the model expects <code>(3 x 64 x 64)</code> images, I had leeway to apply cropping, down-scaling, and horizontal translations to all images before passing them through the network. Given the large image shown below, the model could receive any of the variations presented on the right.</p>
    <center>
    <figure>
    <img alt="nyc_sample_imgs" style="padding-top: 20px;" align="center" width="600" src="./images/translation/nyc_img_transformed_samples.png"> <i>
    <figcaption style="font-size: 12px;">
    Sample Transformations - All images are shifted, center-cropped, and then scaled to <code>3 x 64 x 64</code><sup>4</sup>
    </figcaption>
    </i>
    <figure>
    </center>
    <hr />
    <h3 id="dcgan-results">DCGAN Results</h3>
    <h4 id="results">Results</h4>
    <h4 id="modeling-considerations">Modeling Considerations</h4>
    <p>I do want to stress that this isn’t a strict replication of the original DCGAN paper. Throughout different points in the training process I implemented some of the advice from <a href="https://github.com/soumith/ganhacks">GanHacks</a> to improve the stability of the model. At a low-level, it’s difficult to describe all of the internal consequences of using <code>PyTorch</code> rather than the specific packages the authors used. At a high level, I made the following notable changes:</p>
    <ul>
    <li><p>Choose <code>AdamW</code>/<code>FusedAdamW</code> as an optimizer function over <code>SGD</code>. <em>Goodfellow, et al.</em> use a custom <code>SGD</code> <a href="https://github.com/goodfeli/adversarial/blob/master/sgd.py">implementation</a> that is a patched version of pylearn2’s <code>SGD</code>. Instead, I elected for a built-in PyTorch optimizer, <code>AdamW</code>. As an added benefit, Habana offers their own <code>FusedAdamW</code> implementation that should perform quite well on the Gaudi instances.</p></li>
    <li><p>I add an additional block of <code>Conv2d</code>, <code>BatchNorm2d</code>, and <code>Relu</code> layers to start the model. This allows me to handle for images at <code>(3 x 128 x 128)</code>. It turned out there were significant challenges with using <code>(3 x 64 x 64)</code> images on modern hardware. Although getting stable training on these larger images took a bit of tuning it was an interesting challenge to reason through all of this</p></li>
    <li><p>I remove the final <code>Sigmoid</code> layer from <code>D</code>. Typically a binary classification problem like the one <code>D</code> solves would use <a href="https://en.wikipedia.org/wiki/Cross_entropy">Binary Cross Entropy Loss</a> (<code>BCELoss</code>). The way that PyTorch optimizes for mixed-precision operations required I switch to <code>BCEWithLogitLoss</code>, a loss function that expects logits (<code>L∈(−∞,∞)</code>) rather than probabilities (<code>p∈[0,1]</code>). In effect, this change moves the <code>Sigmoid</code> from the network to part of the loss function.</p></li>
    </ul>
    <hr />
    <h3 id="aws-system-architecture">AWS System Architecture</h3>
    <center>
    <figure>
    <img alt="training_samples_eu" style="padding-top: 20px;" align="center" width="600" src="./images/infra/arch.png"> <i>
    <figcaption style="font-size: 12px;">
    Simplified Model Training Architecture
    </figcaption>
    </i>
    <figure>
    </center>
    <p>All infrastructure for this project is hosted on AWS. If you’d like a user-guide for deploying the architecture yourself, I’d direct you to my infrastructure <a href="https://github.com/DMW2151/msls-infra">repo</a>. All training resources run in a single VPC with two subnets (1 public, 1 private) in the same availability zone. I deployed the following instances to the VPC’s private subnet and accessed them via SSH through a jump-instance deployed to the public subnet.</p>
    <ul>
    <li><p><strong>training-prod</strong> — An EC2 instance for running deep learning models, either <code>DL1</code> or a cost-comparable GPU instance (<code>P</code>-type). In either case, the instance is running a variant of the AWS Deep Learning AMI. Of course, you can construct your own conda environment, container, or AMI for your specific needs.</p></li>
    <li><p><strong>training-nb</strong> — A small Sagemaker instance used for interactive model development, model evaluation, and generating plots.</p></li>
    <li><p><strong>metrics</strong> — A small EC2 instance used to host metrics containers. This machine ran:</p>
    <ul>
    <li><a href="https://www.tensorflow.org/tensorboard">Tensorboard</a> — A tool for visualizing <em>machine learning metrcs</em> during training.</li>
    <li><a href="https://grafana.com/">Grafana</a> — An analytics and monitoring tool. I configured Grafana to visualize <em>machine-level</em> metrics from our training instances.</li>
    </ul></li>
    </ul>
    <p>Each of these instances has access to an AWS Elastic Filesystem (EFS) for saving model data (e.g. checkpoints, plots, traces, etc.). Using EFS saved me hours of data transfer in development and allowed me to pass model checkpoints between machines (i.e. between <em>training-prod</em> and <em>training-nb</em>). Regrettably, an EFS file system can only drive up to 150 KiB/s per GiB of read throughput. With my filesystem using under 100GB, this left me with a paltry ~8MB/s data transfer.</p>
    <p>To alleviate this issue, I downloaded the MSLS data to a <code>gp3</code> volume that I provisioned with high (8000) IOPS and throughput (1000 MiB/s). I can easily attach and detach it from separate training instances as as needed. Anecdotally, this choice led to a <em>2000%</em> speed up in time until first training iteration. Although EBS is more expensive, the decision paid for itself by saving hours of idle GPU/HPU time.</p>
    <hr />
    <h3 id="evaluating-a-first-training-run-on-gpu-instances">Evaluating a First Training run on GPU Instances</h3>
    <p>I started with a PyTorch model running on a GPU (<code>V100</code>) before instrumenting it to run on the HPU. I wanted to make sure that I could do a fair comparison of the two, and that meant ensuring I was optimizing (within reason) for either platform. To validate that the model was sufficiently tuned for the GPU, I referred to the metrics generated by running my model in profiler mode, instance metrics sent to Grafana, and those produced by <code>nvidia-smi</code> (see: <a href="https://developer.nvidia.com/nvidia-system-management-interface">nvidia-smi</a>). With these metrics available, I was able to make choices to improve the model’s training performance.</p>
    <ul>
    <li><p><strong>Batch Size</strong> — This was low-hanging fruit. Independent of the other changes, the right choice of batch size sped up overall execution time by ~80%.</p></li>
    <li><p><strong>Minimize CUDA Copies</strong> — Training statistics, outputs, labels, etc. were being haphazardly moved to and from the GPU! I can collect and display them at the end of the epoch rather than on each batch.</p></li>
    <li><p><strong>Using AMP</strong> — Automatic Mixed Precision (AMP) allows for model training to run with FP16 values where possible and F32 where needed. This allows for lower memory consumption and faster training time. It also opens the door for me to use Habana’s mixed precision <a href="https://docs.habana.ai/en/latest/PyTorch_User_Guide/PyTorch_User_Guide.html#pytorch-mixed-precision-training-on-gaudi">modules</a> when I move over to the <code>DL1</code> instance.</p></li>
    <li><p><strong>Distributed Data Processing</strong> — In isolation, distributed data processing doesn’t improve the model’s training performance, but it does lend towards a more robust training environment. Although this is a problem that uses a moderate amount of small images, I still wanted to instrument my code to run across multiple GPUs (and nodes).</p></li>
    </ul>
    <p>Looking at the first chart below, <em>PyTorch Profiler - GPU Execution Summary</em>, it would seem I was quite close to “perfect” GPU utilization. Unfortunately, the second graph reveals a fundamental problem in my profiling strategy at the time. The sections profiled didn’t include the dataloader steps!</p>
    <table>
    <tbody>
    <tr class="odd">
    <td style="text-align: center;"><em>Figure 1.1 - PyTorch Profiler - GPU Execution Summary</em></td>
    </tr>
    <tr class="even">
    <td style="text-align: center;"><img src="./images/training/big_batch_good.png" alt="OK" /></td>
    </tr>
    </tbody>
    </table>
    <table>
    <tbody>
    <tr class="odd">
    <td style="text-align: center;"><em>Figure 1.2 - Grafana - GPU Utilization Rates</em></td>
    </tr>
    <tr class="even">
    <td style="text-align: center;"><img src="./images/training/gpu_poor.png" alt="Bad GPU" /></td>
    </tr>
    </tbody>
    </table>
    <p>At this point things got quite difficult. I tried tweaking the number of dataloader workers and their pre-fetch factors, no luck. I tried generating an hd5 dataset from my images and writing my own dataloader, again, no luck. I even tried installing a <a href="https://github.com/uploadcare/pillow-simd">SIMD fork of PIL</a> to increase image processing performance. Unfortunately, none of it made a meaningful difference on the <code>V100</code>. I strongly suspected it was the dataloader code that was the bottleneck and did a few sanity checks (see <a href="#Appendix-2---PIL-Benchmarks">Appendix 2</a>) to make sense of things.</p>
    <p>I did some research into <a href="https://pytorch.org/blog/pytorch-profiler-1.9-released/">GPU profiling</a> and learned that GPU utilization is a coarse metric and I was probably already in a OK place from a performance perspective.</p>
    <blockquote>
    <p>Estimated Achieved Occupancy (Est. Achieved Occupancy) is a layer deeper than Est. SM Efficiency and GPU Utilization for diagnosing performance issues. … As a rule of thumb, good throughput gains can be had by improving this metric to 15% and above. But at some point you will hit diminishing returns. If the value is already at 30% for example, further gains will be uncertain.</p>
    </blockquote>
    <p>This low GPU utilization was still a bit unsettling, but my Est. Achieved Occupancy was good and the standard <code>pytorch.DataLoader</code> would stay in the code. Finally, I did a few GPU test runs to collect metrics (<code>p2.8xlarge</code> w. 8 x <code>K80</code> and <code>p3.2xlarge</code> w. 1 x <code>V100</code>) and I moved along to training on the Gaudi-accelerated instances.</p>
    <hr />
    <h3 id="modifications-for-training-on-gaudi-accelerated-instances">Modifications for Training on Gaudi Accelerated Instances</h3>
    <p>Migrating a model to run on HPUs require some changes, most of which are highlighted in the migration <a href="https://docs.habana.ai/en/latest/Migration_Guide/Migration_Guide.html#porting-simple-pyt-model">guide</a>. In general, a few changed imports allow the PyTorch Habana bridge to drive the execution of deep learning models on the Habana Gaudi device. Specifically, I made the following changes for the Gaudi accelerated instances:</p>
    <ul>
    <li><p>Swap out a standard <code>pytorch.DataLoader</code> for <code>habana_dataloader.HabanaDataLoader</code>. Under the right <a href="https://docs.habana.ai/en/v1.1.0/PyTorch_User_Guide/PyTorch_User_Guide.html#habana-data-loader">circumstances</a>, <code>HabanaDataLoader</code> can yield better performance that the native <code>DataLoader</code>. Even without acceleration, I can still use the <code>HabanaDataLoader</code> with a high <code>num_workers</code> parameter to quickly shuttle data onto the device.</p></li>
    <li><p>In the previous section and <a href="#Appendix-2---PIL-Benchmarks">Appendix 2</a> I note that the dataloader was potentially a bottleneck in my training process. Instead of training of of EBS, when training the model on <code>DL1</code>, I’ll be training off <a href="https://aws.amazon.com/ec2/instance-types/dl1/">local storage volumes</a>.</p></li>
    <li><p>Use <code>Lazy Mode</code>. <a href="https://docs.habana.ai/en/v1.1.0/PyTorch_User_Guide/PyTorch_User_Guide.html#lazy-mode">Lazy Mode</a> provides the SynapseAI graph compiler the opportunity to optimize the device execution for multiple ops.</p></li>
    <li><p>Use <code>FusedAdamW</code> over <code>AdamW</code>. <code>FusedAdamW</code> can batch the element-wise updates applied to all the model’s parameters into one or a few kernel launches rather than a single kernel for each parameter. This is a custom optimizer for Habana devices and should yield some performance improvements over <code>AdamW</code>.</p></li>
    </ul>
    <hr />
    <h3 id="comparative-performance">Comparative Performance</h3>
    <table>
    <colgroup>
    <col style="width: 21%" />
    <col style="width: 8%" />
    <col style="width: 20%" />
    <col style="width: 7%" />
    <col style="width: 16%" />
    <col style="width: 8%" />
    <col style="width: 16%" />
    </colgroup>
    <thead>
    <tr class="header">
    <th style="text-align: left;">Model Run</th>
    <th style="text-align: center;">Instance</th>
    <th style="text-align: right;">Average Throughput (Imgs/Hr)</th>
    <th style="text-align: right;">Rate ($)</th>
    <th style="text-align: right;">Throughput / $ (Est.)</th>
    <th style="text-align: right;">Spot Rate ($)</th>
    <th style="text-align: right;">Spot Throughput / $ (Est.)</th>
    </tr>
    </thead>
    <tbody>
    <tr class="odd">
    <td style="text-align: left;">Naive-Params-Multi-GPU</td>
    <td style="text-align: center;">p2.8xlarge</td>
    <td style="text-align: right;">7,780,000</td>
    <td style="text-align: right;">$7.20</td>
    <td style="text-align: right;">1,080,556</td>
    <td style="text-align: right;">$2.16</td>
    <td style="text-align: right;">3,601,852</td>
    </tr>
    <tr class="even">
    <td style="text-align: left;">Naive-Params-Single-GPU</td>
    <td style="text-align: center;">p3.2xlarge</td>
    <td style="text-align: right;">5,830,000</td>
    <td style="text-align: right;">$3.06</td>
    <td style="text-align: right;">1,905,229</td>
    <td style="text-align: right;">$0.92</td>
    <td style="text-align: right;">6,336,957</td>
    </tr>
    <tr class="odd">
    <td style="text-align: left;">Safe-Params-w-Noise-Batch-512</td>
    <td style="text-align: center;">p3.2xlarge</td>
    <td style="text-align: right;">5,020,000</td>
    <td style="text-align: right;">$3.06</td>
    <td style="text-align: right;">1,640,523</td>
    <td style="text-align: right;">$0.92</td>
    <td style="text-align: right;">5,456,522</td>
    </tr>
    </tbody>
    </table>
    <hr />
    <h3 id="appendix-1---modeling-choices">Appendix 1 - Modeling Choices</h3>
    <p>DCGAN is unstable in comparison to modern generative models. After a few test runs, It became clear that the default values suggested in the paper may not be ideal for my use-case or hardware. The most common failure mode I observed involved was <code>D</code> learning the difference between test and real images too quickly; some property of fake images that made them easy to identify, and that left <code>G</code> to make trivial progress in generating better images over time. I didn’t have the time or resources to do a full hyper-parameter tuning, so I used a few <del>tricks</del> heuristics suggested <a href="https://github.com/pytorch/examples/issues/70">here</a> and <a href="https://github.com/soumith/dcgan.torch/issues/2#issuecomment-164862299">here</a> to temper this tendency towards model collapse. I ran ~10 short test runs and treated that as a pseudo grid search. Of the variations I tried the following set of values gave me (anecdotally) the best results on a <code>P3.2xlarge</code> (1 x <code>V100</code>).</p>
    <ul>
    <li><strong>Learning Rate</strong> — 0.0002</li>
    <li><strong>Latent Vector Size</strong> — 256</li>
    <li><strong>Additional Noise Layer in Transformations</strong>: (0, 0.2)</li>
    <li><strong>Batch Size</strong> — 512</li>
    <li><strong>Adam Optimizer w. Weight Decay:</strong> 0.05</li>
    </ul>
    <p>The most transformative of these changes was the addition of an extra noise layer in the dataloader, setting this value high does produce grainy results, but it was an effective remedy against the generator collapsing. There is support for this method in <a href="https://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/">literature</a><sup>5</sup> dating back to at least 2016.</p>
    <p>All of these choices are meant to be conservative in that they forsake optimal performance or faster model-convergence time in favor of stability. Using a more modern model architecture, I may have been able to get away with default parameters. Both networks’ architecture (as presented in the DCGAN paper) is show below. I would advise leaving this unchanged (except for maybe the size of the latent vector, <code>Z</code> passed to the generator).</p>
    <div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Discriminator</span><span class="er">(</span></span>
    <span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">(</span><span class="ex">main</span><span class="kw">)</span><span class="bu">:</span> Sequential<span class="er">(</span></span>
    <span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">0</span><span class="kw">)</span><span class="bu">:</span> Conv2d<span class="er">(</span><span class="ex">3,</span> 64, kernel_size=<span class="er">(</span><span class="ex">4,</span> 4<span class="kw">)</span><span class="ex">,</span> stride=<span class="er">(</span><span class="ex">2,</span> 2<span class="kw">)</span><span class="ex">,</span> padding=<span class="er">(</span><span class="ex">1,</span> 1<span class="kw">)</span><span class="ex">,</span> bias=False<span class="kw">)</span></span>
    <span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">1</span><span class="kw">)</span><span class="bu">:</span> LeakyReLU<span class="er">(</span><span class="va">negative_slope</span><span class="op">=</span>0.2, <span class="va">inplace</span><span class="op">=</span>True<span class="kw">)</span></span>
    <span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">2</span><span class="kw">)</span><span class="bu">:</span> Conv2d<span class="er">(</span><span class="ex">64,</span> 128, kernel_size=<span class="er">(</span><span class="ex">4,</span> 4<span class="kw">)</span><span class="ex">,</span> stride=<span class="er">(</span><span class="ex">2,</span> 2<span class="kw">)</span><span class="ex">,</span> padding=<span class="er">(</span><span class="ex">1,</span> 1<span class="kw">)</span><span class="ex">,</span> bias=False<span class="kw">)</span></span>
    <span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">3</span><span class="kw">)</span><span class="bu">:</span> BatchNorm2d<span class="er">(</span><span class="ex">128,</span> eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<span class="kw">)</span></span>
    <span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">4</span><span class="kw">)</span><span class="bu">:</span> LeakyReLU<span class="er">(</span><span class="va">negative_slope</span><span class="op">=</span>0.2, <span class="va">inplace</span><span class="op">=</span>True<span class="kw">)</span></span>
    <span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">5</span><span class="kw">)</span><span class="bu">:</span> Conv2d<span class="er">(</span><span class="ex">128,</span> 256, kernel_size=<span class="er">(</span><span class="ex">4,</span> 4<span class="kw">)</span><span class="ex">,</span> stride=<span class="er">(</span><span class="ex">2,</span> 2<span class="kw">)</span><span class="ex">,</span> padding=<span class="er">(</span><span class="ex">1,</span> 1<span class="kw">)</span><span class="ex">,</span> bias=False<span class="kw">)</span></span>
    <span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">6</span><span class="kw">)</span><span class="bu">:</span> BatchNorm2d<span class="er">(</span><span class="ex">256,</span> eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<span class="kw">)</span></span>
    <span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">7</span><span class="kw">)</span><span class="bu">:</span> LeakyReLU<span class="er">(</span><span class="va">negative_slope</span><span class="op">=</span>0.2, <span class="va">inplace</span><span class="op">=</span>True<span class="kw">)</span></span>
    <span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">8</span><span class="kw">)</span><span class="bu">:</span> Conv2d<span class="er">(</span><span class="ex">256,</span> 512, kernel_size=<span class="er">(</span><span class="ex">4,</span> 4<span class="kw">)</span><span class="ex">,</span> stride=<span class="er">(</span><span class="ex">2,</span> 2<span class="kw">)</span><span class="ex">,</span> padding=<span class="er">(</span><span class="ex">1,</span> 1<span class="kw">)</span><span class="ex">,</span> bias=False<span class="kw">)</span></span>
    <span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">9</span><span class="kw">)</span><span class="bu">:</span> BatchNorm2d<span class="er">(</span><span class="ex">512,</span> eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<span class="kw">)</span></span>
    <span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">10</span><span class="kw">)</span><span class="bu">:</span> LeakyReLU<span class="er">(</span><span class="va">negative_slope</span><span class="op">=</span>0.2, <span class="va">inplace</span><span class="op">=</span>True<span class="kw">)</span></span>
    <span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">11</span><span class="kw">)</span><span class="bu">:</span> Conv2d<span class="er">(</span><span class="ex">512,</span> 1, kernel_size=<span class="er">(</span><span class="ex">4,</span> 4<span class="kw">)</span><span class="ex">,</span> stride=<span class="er">(</span><span class="ex">1,</span> 1<span class="kw">)</span><span class="ex">,</span> bias=False<span class="kw">)</span></span>
    <span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">12</span><span class="kw">)</span><span class="bu">:</span> <span class="co"># Sigmoid() ## </span><span class="al">NOTE</span><span class="co">: We Removed The Sigmoid Here Because of Our Change from BCELoss to BCEwithLogitLoss</span></span>
    <span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  <span class="kw">)</span></span>
    <span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="kw">)</span></span></code></pre></div>
    <div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Generator</span><span class="er">(</span></span>
    <span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">(</span><span class="ex">main</span><span class="kw">)</span><span class="bu">:</span> Sequential<span class="er">(</span></span>
    <span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">0</span><span class="kw">)</span><span class="bu">:</span> ConvTranspose2d<span class="er">(</span><span class="ex">100,</span> 512, kernel_size=<span class="er">(</span><span class="ex">4,</span> 4<span class="kw">)</span><span class="ex">,</span> stride=<span class="er">(</span><span class="ex">1,</span> 1<span class="kw">)</span><span class="ex">,</span> bias=False<span class="kw">)</span></span>
    <span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">1</span><span class="kw">)</span><span class="bu">:</span> BatchNorm2d<span class="er">(</span><span class="ex">512,</span> eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<span class="kw">)</span></span>
    <span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">2</span><span class="kw">)</span><span class="bu">:</span> ReLU<span class="er">(</span><span class="va">inplace</span><span class="op">=</span>True<span class="kw">)</span></span>
    <span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">3</span><span class="kw">)</span><span class="bu">:</span> ConvTranspose2d<span class="er">(</span><span class="ex">512,</span> 256, kernel_size=<span class="er">(</span><span class="ex">4,</span> 4<span class="kw">)</span><span class="ex">,</span> stride=<span class="er">(</span><span class="ex">2,</span> 2<span class="kw">)</span><span class="ex">,</span> padding=<span class="er">(</span><span class="ex">1,</span> 1<span class="kw">)</span><span class="ex">,</span> bias=False<span class="kw">)</span></span>
    <span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">4</span><span class="kw">)</span><span class="bu">:</span> BatchNorm2d<span class="er">(</span><span class="ex">256,</span> eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<span class="kw">)</span></span>
    <span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">5</span><span class="kw">)</span><span class="bu">:</span> ReLU<span class="er">(</span><span class="va">inplace</span><span class="op">=</span>True<span class="kw">)</span></span>
    <span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">6</span><span class="kw">)</span><span class="bu">:</span> ConvTranspose2d<span class="er">(</span><span class="ex">256,</span> 128, kernel_size=<span class="er">(</span><span class="ex">4,</span> 4<span class="kw">)</span><span class="ex">,</span> stride=<span class="er">(</span><span class="ex">2,</span> 2<span class="kw">)</span><span class="ex">,</span> padding=<span class="er">(</span><span class="ex">1,</span> 1<span class="kw">)</span><span class="ex">,</span> bias=False<span class="kw">)</span></span>
    <span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">7</span><span class="kw">)</span><span class="bu">:</span> BatchNorm2d<span class="er">(</span><span class="ex">128,</span> eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<span class="kw">)</span></span>
    <span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">8</span><span class="kw">)</span><span class="bu">:</span> ReLU<span class="er">(</span><span class="va">inplace</span><span class="op">=</span>True<span class="kw">)</span></span>
    <span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">9</span><span class="kw">)</span><span class="bu">:</span> ConvTranspose2d<span class="er">(</span><span class="ex">128,</span> 64, kernel_size=<span class="er">(</span><span class="ex">4,</span> 4<span class="kw">)</span><span class="ex">,</span> stride=<span class="er">(</span><span class="ex">2,</span> 2<span class="kw">)</span><span class="ex">,</span> padding=<span class="er">(</span><span class="ex">1,</span> 1<span class="kw">)</span><span class="ex">,</span> bias=False<span class="kw">)</span></span>
    <span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">10</span><span class="kw">)</span><span class="bu">:</span> BatchNorm2d<span class="er">(</span><span class="ex">64,</span> eps=1e-05, momentum=0.1, affine=True, track_running_stats=True<span class="kw">)</span></span>
    <span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">11</span><span class="kw">)</span><span class="bu">:</span> ReLU<span class="er">(</span><span class="va">inplace</span><span class="op">=</span>True<span class="kw">)</span></span>
    <span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">12</span><span class="kw">)</span><span class="bu">:</span> ConvTranspose2d<span class="er">(</span><span class="ex">64,</span> 3, kernel_size=<span class="er">(</span><span class="ex">4,</span> 4<span class="kw">)</span><span class="ex">,</span> stride=<span class="er">(</span><span class="ex">2,</span> 2<span class="kw">)</span><span class="ex">,</span> padding=<span class="er">(</span><span class="ex">1,</span> 1<span class="kw">)</span><span class="ex">,</span> bias=False<span class="kw">)</span></span>
    <span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">(</span><span class="ex">13</span><span class="kw">)</span><span class="bu">:</span> Tanh<span class="er">(</span><span class="kw">)</span></span>
    <span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>  <span class="kw">)</span></span>
    <span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="kw">)</span></span></code></pre></div>
    <h3 id="appendix-2---comparable-instance-selection">Appendix 2 - Comparable Instance Selection</h3>
    <p>Using <a href="https://instances.vantage.sh/">instances.vantage.sh</a> and <code>aws describe-instances</code>, I aggregated data for all EC2 instances available in <code>us-east-1</code> with between 2 and 8 GPUs. These machines range from those with GPUs that are designed for graphics workloads (e.g. <code>G3</code> instances with Tesla <code>M60</code>s) to top-of-the line training instances (e.g. <code>P4</code> instances with <code>A100</code>s). I relied exclusively on Nvidia’s most recent <a href="https://developer.nvidia.com/deep-learning-performance-training-inference">resnext-101 benchmarks</a> as a proxy for my model’s performance. On price, <code>p3.8xlarge</code> instances are the most similar to the <code>DL1</code> and offer 4 <code>V100</code>. Although <code>g4dn.12xlarge</code>(<code>T4</code>) and <code>p2.8xlarge</code> (<code>K80</code>) instances are priced well relative to their performance, I elected to only run a full test on the <code>p3.8xlarge</code>.</p>
    <table>
    <caption>Table A.1.1 - Possible Comparable GPU Instances</caption>
    <thead>
    <tr class="header">
    <th>API Name</th>
    <th>Memory (GiB)</th>
    <th>VCPUs</th>
    <th>GPUs</th>
    <th>GPU Model</th>
    <th>GPU Mem (GiB)</th>
    <th>$/Hr</th>
    </tr>
    </thead>
    <tbody>
    <tr class="odd">
    <td>g3.8xlarge</td>
    <td>244</td>
    <td>32</td>
    <td>2</td>
    <td>NVIDIA Tesla M60</td>
    <td>16</td>
    <td>2.28</td>
    </tr>
    <tr class="even">
    <td>g3.16xlarge</td>
    <td>488</td>
    <td>64</td>
    <td>4</td>
    <td>NVIDIA Tesla M60</td>
    <td>32</td>
    <td>4.56</td>
    </tr>
    <tr class="odd">
    <td>p2.8xlarge</td>
    <td>488</td>
    <td>32</td>
    <td>8</td>
    <td>NVIDIA Tesla K80</td>
    <td>96</td>
    <td>7.20</td>
    </tr>
    <tr class="even">
    <td>g4dn.12xlarge</td>
    <td>192</td>
    <td>48</td>
    <td>4</td>
    <td>NVIDIA T4 Tensor Core</td>
    <td>64</td>
    <td>3.91</td>
    </tr>
    <tr class="odd">
    <td>g4dn.metal</td>
    <td>384</td>
    <td>96</td>
    <td>8</td>
    <td>NVIDIA T4 Tensor Core</td>
    <td>128</td>
    <td>7.82</td>
    </tr>
    <tr class="even">
    <td>g5.12xlarge</td>
    <td>192</td>
    <td>48</td>
    <td>4</td>
    <td>NVIDIA A10G</td>
    <td>96</td>
    <td>5.67</td>
    </tr>
    <tr class="odd">
    <td>g5.24xlarge</td>
    <td>384</td>
    <td>96</td>
    <td>4</td>
    <td>NVIDIA A10G</td>
    <td>96</td>
    <td>8.14</td>
    </tr>
    <tr class="even">
    <td>g5.48xlarge</td>
    <td>768</td>
    <td>192</td>
    <td>8</td>
    <td>NVIDIA A10G</td>
    <td>192</td>
    <td>16.29</td>
    </tr>
    <tr class="odd">
    <td>p3.8xlarge</td>
    <td>244</td>
    <td>32</td>
    <td>4</td>
    <td>NVIDIA Tesla V100</td>
    <td>64</td>
    <td>12.24</td>
    </tr>
    <tr class="even">
    <td>p3.16xlarge</td>
    <td>488</td>
    <td>64</td>
    <td>8</td>
    <td>NVIDIA Tesla V100</td>
    <td>128</td>
    <td>24.48</td>
    </tr>
    <tr class="odd">
    <td>p3dn.24xlarge</td>
    <td>768</td>
    <td>96</td>
    <td>8</td>
    <td>NVIDIA Tesla V100</td>
    <td>256</td>
    <td>31.21</td>
    </tr>
    <tr class="even">
    <td>p4d.24xlarge</td>
    <td>1152</td>
    <td>96</td>
    <td>8</td>
    <td>NVIDIA A100</td>
    <td>320</td>
    <td>32.77</td>
    </tr>
    </tbody>
    </table>
    <h3 id="appendix-3---pil-benchmarks">Appendix 3 - PIL Benchmarks</h3>
    <p>I narrowed down the source of the drops in GPU utilization to the dataloader being slow relative to the GPU. Every batch is doing thousands of <code>PIL.open()</code> calls (<a href="https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L245-L249">source</a>), if these calls are causing the slowdown, we should be able to see a huge amount of stress on the disk during the loader step.</p>
    <ul>
    <li><strong>Let’s just use a worse GPU!</strong> — I spun up a <code>p2.8xlarge</code> with <code>K80</code>s to see if the weaker GPU would produce nicer utilization metrics. In theory, if the GPU is the bottleneck instead of the dataloader, I won’t see these periodic dips. This is a bit of a vanity metric and I have no interest in doubling my training costs for vanity’s sake, but the charts below confirm my hypothesis. This was an excellent discovery!</li>
    </ul>
    <table>
    <tbody>
    <tr class="odd">
    <td style="text-align: center;"><em>Figure A2.1.1 - GPU Training - GPU Usage - P2.8xLarge</em></td>
    </tr>
    <tr class="even">
    <td style="text-align: center;"><img src="./images/training/vanity_gpu.png" alt="OK" /></td>
    </tr>
    </tbody>
    </table>
    <ul>
    <li><strong>Why not profile the disk?</strong> — Back on the <code>p3.2xlarge</code>, I figured I should profile the disk to see what was going on during the utilization drops. I thought a maxed-out <code>gp3</code> would have been adequate, but maybe I should have sprung for the <code>io1</code> or <code>io2</code>. In <em>Figure A2.1 - GPU Training - Atop + Nvidia SMI Profile</em> , you can see the results of <code>atop</code> and <code>nvidia-smi</code> during a training run. When the GPU is at low utilization. the disk where <code>MSLS</code> is mounted (<code>/dev/xvdh</code>) is <strong>working!</strong>.</li>
    </ul>
    <table>
    <tbody>
    <tr class="odd">
    <td style="text-align: center;"><em>Figure A2.1 - GPU Training - Atop + Nvidia SMI Profile</em></td>
    </tr>
    <tr class="even">
    <td style="text-align: center;"><img src="./images/training/disk_saturated.png" alt="OK" /></td>
    </tr>
    </tbody>
    </table>
    <p>Thinking about it in retrospect, this all makes sense. We’re opening images that are <code>(3 x 360 x 480)</code> and the GPU is doing some light calculations to resize and re-color them, but then running expensive convolutions on images that are just <code>(3 x 64 x 64)</code>.</p>
    <h3 id="appendix-4---assessing-gan-performance">Appendix 4 - Assessing GAN performance</h3>
    <p>In <em>Goodfellow, et al.</em>, the authors use the procedure described below to estimate the relative performance of multiple generative methods. Rather than using this procedure to evaluate other models, I implemented it for comparing intra-model progress across epochs.</p>
    <blockquote>
    <p>We estimate probability of the test set data under <em>Pg</em> by fitting a Gaussian Parzen window to the samples generated with G and reporting the log-likelihood under this distribution. The σ parameter of the Gaussians was obtained by cross validation on the validation set. This procedure was introduced in Breuleux et al. [7] and used for various generative models for which the exact likelihood is not tractable.</p>
    </blockquote>
    <p>This method of estimation has some significant flaws…</p>
    <hr />
    <h3 id="citations">Citations</h3>
    <p><strong><sup>1</sup></strong> <em>“Generative Adversarial Networks.” Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. ArXiv 2014.</em></p>
    <p><strong><sup>2</sup></strong> <em>Radford, Alec, Luke Metz, and Soumith Chintala. “Unsupervised representation learning with deep convolutional generative adversarial networks.” arXiv preprint arXiv:1511.06434 (2015).</em></p>
    <p><strong><sup>3</sup></strong> <em>F. Warburg, S. Hauberg, M. Lopez-Antequera, P. Gargallo, Y. Kuang, and J. Civera. Mapillary Street-Level Sequences: A Dataset for Lifelong Place Recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020</em></p>
    <p><strong><sup>4</sup></strong> <em>File:NYC 14th Street looking west 12 2005.jpg. (2020, September 13). Wikimedia Commons, the free media repository. Retrieved 23:09, January 25, 2022 from https://commons.wikimedia.org/w/index.php?title=File:NYC_14th_Street_looking_west_12_2005.jpg&amp;oldid=457344851</em></p>
    <p><strong><sup>5</sup></strong> <em>Sønderby, Casper Kaae, et al. “Amortised map inference for image super-resolution.” arXiv preprint arXiv:1610.04490 (2016).</em></p>
    <p><strong><sup>6</sup></strong> <em>Salimans, Tim, et al. “Improved techniques for training gans.” Advances in neural information processing systems 29 (2016).</em></p>
    <footer class="site-footer">
        <span class="site-footer-owner"> Maintained by <a href="https://github.com/DMW2151">DMW2151</a>.</span>
    </footer>
  </body>
</html>
